{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4be94-868d-4299-9e65-5251b94fce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of Today :  4  / 3 \n",
      "Hour :  10\n",
      "Training PDE\n",
      "\n",
      "\n",
      "Training Pass 1\n",
      "\n",
      "\n",
      "Current End Time: 0.1 Current Learning Rate:  0.001\n",
      "\n",
      "Iteration: 1 \tTotal Loss: tensor(5.5932)\n",
      "\tIC Loss:  1.2571160793304443 \t BC u Loss:  0.00039602266042493284 \t BC u_x Loss:  0.003966374322772026 \n",
      "AC PDE Loss:  2.68796443939209 \tadaptive PDE Loss:  1.6437525749206543\n",
      "\n",
      "Iteration: 500 \tTotal Loss: tensor(1.3421)\n",
      "\tIC Loss:  1.2429651021957397 \t BC u Loss:  0.01983959972858429 \t BC u_x Loss:  0.015772409737110138 \n",
      "AC PDE Loss:  0.0011332143330946565 \tadaptive PDE Loss:  0.06239516660571098\n",
      "\n",
      "Iteration: 1000 \tTotal Loss: tensor(1.3156)\n",
      "\tIC Loss:  1.223166584968567 \t BC u Loss:  0.014729511924088001 \t BC u_x Loss:  0.012681097723543644 \n",
      "AC PDE Loss:  0.0013836306752637029 \tadaptive PDE Loss:  0.0636768788099289\n",
      "\n",
      "Iteration: 1500 \tTotal Loss: tensor(1.2523)\n",
      "\tIC Loss:  1.198140025138855 \t BC u Loss:  0.0067271701991558075 \t BC u_x Loss:  0.007113310508430004 \n",
      "AC PDE Loss:  0.0005667762598022819 \tadaptive PDE Loss:  0.03979295492172241\n",
      "\n",
      "Iteration: 2000 \tTotal Loss: tensor(1.2186)\n",
      "\tIC Loss:  1.1130183935165405 \t BC u Loss:  0.005358589813113213 \t BC u_x Loss:  0.007204888854175806 \n",
      "AC PDE Loss:  0.004490200895816088 \tadaptive PDE Loss:  0.08857743442058563\n",
      "\n",
      "Iteration: 2500 \tTotal Loss: tensor(1.1843)\n",
      "\tIC Loss:  1.111921787261963 \t BC u Loss:  0.005850302986800671 \t BC u_x Loss:  0.00598118593916297 \n",
      "AC PDE Loss:  0.0022460182663053274 \tadaptive PDE Loss:  0.05826213210821152\n",
      "\n",
      "Iteration: 3000 \tTotal Loss: tensor(1.2213)\n",
      "\tIC Loss:  1.104292869567871 \t BC u Loss:  0.005642653908580542 \t BC u_x Loss:  0.0052736252546310425 \n",
      "AC PDE Loss:  0.006805881392210722 \tadaptive PDE Loss:  0.09924463927745819\n",
      "\n",
      "Iteration: 3500 \tTotal Loss: tensor(1.1596)\n",
      "\tIC Loss:  1.1054521799087524 \t BC u Loss:  0.006925145164132118 \t BC u_x Loss:  0.0061767627485096455 \n",
      "AC PDE Loss:  0.0007115045445971191 \tadaptive PDE Loss:  0.04036593437194824\n",
      "\n",
      "Iteration: 4000 \tTotal Loss: tensor(1.2316)\n",
      "\tIC Loss:  1.1034151315689087 \t BC u Loss:  0.008422978222370148 \t BC u_x Loss:  0.0051659224554896355 \n",
      "AC PDE Loss:  0.008320311084389687 \tadaptive PDE Loss:  0.1063084751367569\n",
      "\n",
      "Iteration: 4500 \tTotal Loss: tensor(1.1772)\n",
      "\tIC Loss:  1.0956270694732666 \t BC u Loss:  0.009332217276096344 \t BC u_x Loss:  0.004500743467360735 \n",
      "AC PDE Loss:  0.0020499220117926598 \tadaptive PDE Loss:  0.06568924337625504\n",
      "\n",
      "Iteration: 5000 \tTotal Loss: tensor(1.1279)\n",
      "\tIC Loss:  1.092250943183899 \t BC u Loss:  0.008420700207352638 \t BC u_x Loss:  0.004317482467740774 \n",
      "AC PDE Loss:  0.0002917815581895411 \tadaptive PDE Loss:  0.02259085141122341\n",
      "\n",
      "Iteration: 5500 \tTotal Loss: tensor(1.1850)\n",
      "\tIC Loss:  1.092595100402832 \t BC u Loss:  0.006383106112480164 \t BC u_x Loss:  0.0035529069136828184 \n",
      "AC PDE Loss:  0.004209680482745171 \tadaptive PDE Loss:  0.07828351855278015\n",
      "\n",
      "Iteration: 6000 \tTotal Loss: tensor(1.1530)\n",
      "\tIC Loss:  1.087562084197998 \t BC u Loss:  0.0037589510902762413 \t BC u_x Loss:  0.0041532330214977264 \n",
      "AC PDE Loss:  0.0017125424928963184 \tadaptive PDE Loss:  0.05577455461025238\n",
      "\n",
      "Iteration: 6500 \tTotal Loss: tensor(1.1269)\n",
      "\tIC Loss:  1.0846216678619385 \t BC u Loss:  0.0037764213047921658 \t BC u_x Loss:  0.0049835448153316975 \n",
      "AC PDE Loss:  0.0009447459597140551 \tadaptive PDE Loss:  0.03257053717970848\n",
      "\n",
      "Iteration: 7000 \tTotal Loss: tensor(1.1187)\n",
      "\tIC Loss:  1.0834051370620728 \t BC u Loss:  0.0033612262923270464 \t BC u_x Loss:  0.005872318521142006 \n",
      "AC PDE Loss:  0.0005473237251862884 \tadaptive PDE Loss:  0.025545481592416763\n",
      "\n",
      "Iteration: 7500 \tTotal Loss: tensor(1.1324)\n",
      "\tIC Loss:  1.08230459690094 \t BC u Loss:  0.0030172027181833982 \t BC u_x Loss:  0.007480303756892681 \n",
      "AC PDE Loss:  0.00106119888368994 \tadaptive PDE Loss:  0.0385034941136837\n",
      "\n",
      "Iteration: 8000 \tTotal Loss: tensor(1.1240)\n",
      "\tIC Loss:  1.0841259956359863 \t BC u Loss:  0.002689928514882922 \t BC u_x Loss:  0.006710115820169449 \n",
      "AC PDE Loss:  0.0006979548488743603 \tadaptive PDE Loss:  0.029797419905662537\n",
      "\n",
      "Iteration: 8500 \tTotal Loss: tensor(1.1223)\n",
      "\tIC Loss:  1.0823866128921509 \t BC u Loss:  0.0031259432435035706 \t BC u_x Loss:  0.008943509310483932 \n",
      "AC PDE Loss:  0.0006247624405659735 \tadaptive PDE Loss:  0.027240533381700516\n",
      "\n",
      "Iteration: 9000 \tTotal Loss: tensor(1.1175)\n",
      "\tIC Loss:  1.0824363231658936 \t BC u Loss:  0.0028527912218123674 \t BC u_x Loss:  0.008287318982183933 \n",
      "AC PDE Loss:  0.0005027297884225845 \tadaptive PDE Loss:  0.023414522409439087\n",
      "\n",
      "Iteration: 9500 \tTotal Loss: tensor(1.1233)\n",
      "\tIC Loss:  1.0823732614517212 \t BC u Loss:  0.002769760088995099 \t BC u_x Loss:  0.008295588195323944 \n",
      "AC PDE Loss:  0.0007408717647194862 \tadaptive PDE Loss:  0.02916061319410801\n",
      "\n",
      "Iteration: 10000 \tTotal Loss: tensor(1.1162)\n",
      "\tIC Loss:  1.0870558023452759 \t BC u Loss:  0.002156186616048217 \t BC u_x Loss:  0.0020161631982773542 \n",
      "AC PDE Loss:  0.00044559233356267214 \tadaptive PDE Loss:  0.024564053863286972\n",
      "\n",
      "Iteration: 10500 \tTotal Loss: tensor(1.1124)\n",
      "\tIC Loss:  1.0816895961761475 \t BC u Loss:  0.002857379848137498 \t BC u_x Loss:  0.006827260833233595 \n",
      "AC PDE Loss:  0.00039778740028850734 \tadaptive PDE Loss:  0.02063201554119587\n",
      "\n",
      "Iteration: 11000 \tTotal Loss: tensor(1.1148)\n",
      "\tIC Loss:  1.0835456848144531 \t BC u Loss:  0.0028234904166311026 \t BC u_x Loss:  0.008513549342751503 \n",
      "AC PDE Loss:  0.0003536709409672767 \tadaptive PDE Loss:  0.01953207328915596\n",
      "\n",
      "Iteration: 11500 \tTotal Loss: tensor(1.1243)\n",
      "\tIC Loss:  1.08904230594635 \t BC u Loss:  0.002498268848285079 \t BC u_x Loss:  0.0007143885013647377 \n",
      "AC PDE Loss:  0.0008693636045791209 \tadaptive PDE Loss:  0.031204070895910263\n",
      "\n",
      "Iteration: 12000 \tTotal Loss: tensor(1.1240)\n",
      "\tIC Loss:  1.0850847959518433 \t BC u Loss:  0.0029630798380821943 \t BC u_x Loss:  0.0024129122029989958 \n",
      "AC PDE Loss:  0.0009069552179425955 \tadaptive PDE Loss:  0.03258691728115082\n",
      "\n",
      "Iteration: 12500 \tTotal Loss: tensor(1.1264)\n",
      "\tIC Loss:  1.0821380615234375 \t BC u Loss:  0.0029820692725479603 \t BC u_x Loss:  0.004464507568627596 \n",
      "AC PDE Loss:  0.0009902222082018852 \tadaptive PDE Loss:  0.03577708080410957\n",
      "\n",
      "Iteration: 13000 \tTotal Loss: tensor(1.1418)\n",
      "\tIC Loss:  1.0817872285842896 \t BC u Loss:  0.003288327483460307 \t BC u_x Loss:  0.003867954947054386 \n",
      "AC PDE Loss:  0.002276140497997403 \tadaptive PDE Loss:  0.05058680847287178\n",
      "\n",
      "Iteration: 13500 \tTotal Loss: tensor(1.1023)\n",
      "\tIC Loss:  1.0840882062911987 \t BC u Loss:  0.0024422246497124434 \t BC u_x Loss:  0.003502489998936653 \n",
      "AC PDE Loss:  0.00011185189941897988 \tadaptive PDE Loss:  0.012148784473538399\n",
      "\n",
      "Iteration: 14000 \tTotal Loss: tensor(1.1022)\n",
      "\tIC Loss:  1.0818036794662476 \t BC u Loss:  0.0028046593070030212 \t BC u_x Loss:  0.008365850895643234 \n",
      "AC PDE Loss:  7.735314284218475e-05 \tadaptive PDE Loss:  0.009172063320875168\n",
      "\n",
      "Iteration: 14500 \tTotal Loss: tensor(1.1061)\n",
      "\tIC Loss:  1.080707311630249 \t BC u Loss:  0.002987668616697192 \t BC u_x Loss:  0.010693412274122238 \n",
      "AC PDE Loss:  0.00012022874579997733 \tadaptive PDE Loss:  0.011569825001060963\n",
      "\n",
      "Iteration: 15000 \tTotal Loss: tensor(1.1038)\n",
      "\tIC Loss:  1.085247278213501 \t BC u Loss:  0.002506644232198596 \t BC u_x Loss:  0.004903341643512249 \n",
      "AC PDE Loss:  0.00010671629570424557 \tadaptive PDE Loss:  0.011079899966716766\n",
      "\n",
      "Iteration: 15500 \tTotal Loss: tensor(1.1074)\n",
      "\tIC Loss:  1.0840462446212769 \t BC u Loss:  0.002922530984506011 \t BC u_x Loss:  0.010187648236751556 \n",
      "AC PDE Loss:  9.162454080069438e-05 \tadaptive PDE Loss:  0.010137286968529224\n",
      "\n",
      "Iteration: 16000 \tTotal Loss: tensor(1.1429)\n",
      "\tIC Loss:  1.0810575485229492 \t BC u Loss:  0.004366125445812941 \t BC u_x Loss:  0.00999757181853056 \n",
      "AC PDE Loss:  0.0013646006118506193 \tadaptive PDE Loss:  0.04609110951423645\n",
      "\n",
      "Iteration: 16500 \tTotal Loss: tensor(1.1191)\n",
      "\tIC Loss:  1.0812876224517822 \t BC u Loss:  0.0029553405474871397 \t BC u_x Loss:  0.009087434969842434 \n",
      "AC PDE Loss:  0.0005853651091456413 \tadaptive PDE Loss:  0.025164222344756126\n",
      "\n",
      "Iteration: 17000 \tTotal Loss: tensor(1.1970)\n",
      "\tIC Loss:  1.0781995058059692 \t BC u Loss:  0.005988717079162598 \t BC u_x Loss:  0.020201221108436584 \n",
      "AC PDE Loss:  0.006440029479563236 \tadaptive PDE Loss:  0.08614270389080048\n",
      "\n",
      "Iteration: 17500 \tTotal Loss: tensor(1.1160)\n",
      "\tIC Loss:  1.081478476524353 \t BC u Loss:  0.0029141881968826056 \t BC u_x Loss:  0.008498196490108967 \n",
      "AC PDE Loss:  0.0004835761501453817 \tadaptive PDE Loss:  0.022633995860815048\n",
      "\n",
      "Iteration: 18000 \tTotal Loss: tensor(1.1179)\n",
      "\tIC Loss:  1.081084966659546 \t BC u Loss:  0.0029174976516515017 \t BC u_x Loss:  0.010930918157100677 \n",
      "AC PDE Loss:  0.0004826560616493225 \tadaptive PDE Loss:  0.022448133677244186\n",
      "\n",
      "Iteration: 18500 \tTotal Loss: tensor(1.1043)\n",
      "\tIC Loss:  1.0887006521224976 \t BC u Loss:  3.860817443523956e-08 \t BC u_x Loss:  1.3692510947294068e-07 \n",
      "AC PDE Loss:  0.00015212090511340648 \tadaptive PDE Loss:  0.015415579080581665\n",
      "\n",
      "Iteration: 19000 \tTotal Loss: tensor(1.1244)\n",
      "\tIC Loss:  1.0881165266036987 \t BC u Loss:  4.762212029163493e-06 \t BC u_x Loss:  9.47957712327252e-10 \n",
      "AC PDE Loss:  0.0008932208293117583 \tadaptive PDE Loss:  0.03540351614356041\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from scipy.stats import qmc\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from Building_Net import Net\n",
    "from Conditions import lossIC, lossBdry,lossNSpde, lossNSpde_rank\n",
    "\n",
    "currentDateTime = datetime.now()\n",
    "print(\"Date of Today : \", currentDateTime.month, \" /\", currentDateTime.day, \"\\nHour : \", currentDateTime.hour) \n",
    "ctime = f\"{currentDateTime.month}_{currentDateTime.day}_{currentDateTime.hour}h\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_network(load, loadfile):\n",
    "    \n",
    "    net = Net()\n",
    "    net = net.to(device)\n",
    "    epsilon = []\n",
    "    #Attempt to load the saved pt. file\n",
    "    if load == True:\n",
    "        try:\n",
    "            net.load_state_dict(torch.load(loadfile, map_location=torch.device(device)))\n",
    "        except:\n",
    "            print(\"\\nLoading file was failed\\n\")\n",
    "        else:\n",
    "            print(\"\\nLoading file was completed\\n\")\n",
    "    \n",
    "    print('Training PDE')\n",
    "    start = time.time() #initialize tracking computational time\n",
    "    \n",
    "    partial_time_set = [0, 0, 0] #initialize time recording list with number of learnning rates on whole domain training\n",
    "    \n",
    "    for i in range(len(partial_time_set)):\n",
    "        if i == 0:\n",
    "            #First loop uses progressively increasing time intervals\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            time_slices = np.array([.1, .2, .3, .4, .5, .6, .7, .8, .9, 1])\n",
    "            iterations = 30000 #iterations for each learning rate\n",
    "            learning_rate = 10**-3\n",
    "        elif i == 1:\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            #time_slices = [time_slices[-1]]\n",
    "            time_slices = [time_slices[-1]]\n",
    "            iterations = 30000 #iterations for each learning rate\n",
    "            learning_rate = 10**-4\n",
    "        elif i == 2:\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            time_slices = [time_slices[-1]]\n",
    "            iterations = 0 #iterations for each learning rate\n",
    "            learning_rate = 10**-4\n",
    "        \n",
    "        training_loop(net, time_slices, iterations, learning_rate, record_loss = 100, print_loss = 500, epsilon = epsilon)\n",
    "        torch.save(net.state_dict(), f\"{ctime}_Training_{i+1}.pt\")\n",
    "        partial_time_set[i] = time.time()\n",
    "        np.savetxt(f\"{ctime}epsilon_{i}.txt\", epsilon)\n",
    "\n",
    "    print(\"Total Time:\\t\", partial_time_set[-1]-start, '\\nPass 1 Time:\\t', partial_time_set[0]-start, \n",
    "          '\\nPass 2 Time:\\t', partial_time_set[1]-partial_time_set[0], '\\nPass 3 Time:\\t', partial_time_set[2]-partial_time_set[1])\n",
    "   \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def exp_time_sample(collocation_pt_number, t_l, t_u, r):\n",
    "    t_uni = np.random.uniform(0,1, size=(collocation, 1))\n",
    "    \n",
    "    t = -np.log(1-t_uni+t_uni*np.exp(-r*(t_u-t_l)))/r\n",
    "    \n",
    "    return t\n",
    "def training_loop(net, time_slices, iterations, learning_rate, record_loss, print_loss, epsilon):\n",
    "    \n",
    "    # Domain boundary values\n",
    "    x_l = net.x1_l\n",
    "    x_u = net.x1_u\n",
    "    \n",
    "    #time starts at 0, ends at upper bouund updated in time_slices\n",
    "    t_l = 0\n",
    "\n",
    "    #numbers of sampling collocation points on each part\n",
    "    IC_collocation = int(200)\n",
    "    BC_collocation = int(200)\n",
    "    pde_collocation = int(2000)\n",
    "\n",
    "    #sampler setting for LatinHypercube sampling\n",
    "    IC_lh_sampler = qmc.LatinHypercube(d=1)\n",
    "    BC_lh_sampler = qmc.LatinHypercube(d=1)\n",
    "    PDE_lh_sampler = qmc.LatinHypercube(d=2)\n",
    "    \n",
    "    #update the learning rate as defined\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    #Iterate over time slices\n",
    "    for final_time in time_slices:\n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"\\n\\nCurrent End Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        epoch = 0\n",
    "        for epoch in range(1, iterations):\n",
    "\n",
    "            # initialize gradients to zero\n",
    "            net.optimizer.zero_grad()\n",
    "\n",
    "            ##Define input points with LatinHypercube sampling\n",
    "            x_IC = x_l + (x_u-x_l) *IC_lh_sampler.random(n=IC_collocation).reshape(IC_collocation,1)\n",
    "            t_IC = np.random.uniform(low=t_l, high=t_l, size=(IC_collocation,1))\n",
    "            \n",
    "            input_x_IC = Variable(torch.from_numpy(x_IC).float(), requires_grad=True).to(device)\n",
    "            input_t_IC = Variable(torch.from_numpy(t_IC).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            t_BC = t_l + (final_time-t_l) *BC_lh_sampler.random(n=BC_collocation).reshape(BC_collocation,1)\n",
    "    \n",
    "            input_t_BC = Variable(torch.from_numpy(t_BC).float(), requires_grad=True).to(device)\n",
    "    \n",
    "            x_domain = x_l + (x_u-x_l) *PDE_lh_sampler.random(n=pde_collocation)[:,0].reshape(pde_collocation,1)\n",
    "            t_domain = t_l + (final_time-t_l) *PDE_lh_sampler.random(n=pde_collocation)[:,1].reshape(pde_collocation,1)\n",
    "            t_domain_exp = exp_time_sample(pde_collocation, t_l, final_time, r=10)\n",
    "            \n",
    "            input_x_domain = Variable(torch.from_numpy(x_domain).float(), requires_grad=True).to(device)\n",
    "            input_t_domain = Variable(torch.from_numpy(t_domain).float(), requires_grad=True).to(device)\n",
    "            input_t_domain_exp = Variable(torch.from_numpy(t_domain_exp).float(), requires_grad=True).to(device)\n",
    "\n",
    "            x_domain_r = x_l + (x_u-x_l) *PDE_lh_sampler.random(n=pde_collocation)[:,0].reshape(pde_collocation,1)\n",
    "            t_domain_r = t_l + (final_time-t_l) *PDE_lh_sampler.random(n=pde_collocation)[:,1].reshape(pde_collocation,1)\n",
    "            t_domain_r_exp = exp_time_sample(pde_collocation, t_l, final_time, r=10)\n",
    "            \n",
    "            input_x_domain_r = Variable(torch.from_numpy(x_domain_r).float(), requires_grad=True).to(device)\n",
    "            input_t_domain_r = Variable(torch.from_numpy(t_domain_r).float(), requires_grad=True).to(device)\n",
    "            input_t_domain_r_exp = Variable(torch.from_numpy(t_domain_r_exp).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            #Take additive appaptive sampling with 500 highest loss points\n",
    "            PDEloss_tensor= lossNSpde_rank(net, input_x_domain_r, input_t_domain_r)     \n",
    "            \n",
    "            sorted_tensor, indices = torch.sort(PDEloss_tensor.view(-1), descending=True)\n",
    "            sorted_tensor = sorted_tensor.view(-1, 1)\n",
    "            \n",
    "            max_stad = sorted_tensor[int(pde_collocation/10)-1,0].item()\n",
    "            \n",
    "            PDEloss_picked = torch.where(PDEloss_tensor>=max_stad, PDEloss_tensor, 0)\n",
    "            PDEloss_adaptive = (torch.sum(PDEloss_picked**2)/(int(pde_collocation/10)))**.5\n",
    "\n",
    "            \n",
    "            #Loss computation\n",
    "            u_IC_loss = lossIC(net, input_x_IC, input_t_IC) #, u_IC_loss_mesh\n",
    "            mse_IC = u_IC_loss\n",
    "            \n",
    "            #Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "            mse_BC_u, mse_BC_u_x = lossBdry(net, input_t_BC) \n",
    "            mse_BC = mse_BC_u+ mse_BC_u_x \n",
    "            \n",
    "            #Loss based on PDE\n",
    "            AC_mse= lossNSpde(net, input_x_domain, input_t_domain) \n",
    "            mse_PDE = AC_mse + PDEloss_adaptive\n",
    "        \n",
    "            loss =  (mse_BC + mse_IC + mse_PDE )\n",
    "        \n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            def closure():\n",
    "                return loss\n",
    "            \n",
    "            #Make Iterative Step\n",
    "            net.optimizer.step() #net.optimizer.step(closure)\n",
    "            \n",
    "            \n",
    "            # Gradient Norm Clipping\n",
    "            #torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm = 1000, error_if_nonfinite=False) #note max norm is more art than science\n",
    "            \n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch == 1 or epoch%print_loss == 0:\n",
    "                    print(\"\\nIteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    #,\"\\tadaptive IC Loss2: \", mse_IC_a2.item() , \"\\tadaptive CH PDE Loss2: \", CH_a2.item()\n",
    "                    print(\"\\tIC Loss: \", mse_IC.item(),\n",
    "                          \"\\t BC u Loss: \", mse_BC_u.item(), \"\\t BC u_x Loss: \", mse_BC_u_x.item(),\n",
    "                          \"\\nAC PDE Loss: \", AC_mse.item(), \"\\tadaptive PDE Loss: \", PDEloss_adaptive.item()\n",
    "                         ) \n",
    "                if epoch%3000 == 0:\n",
    "                    np.savetxt(f\"{ctime}epsilon.txt\", epsilon)\n",
    "                    torch.save(net.state_dict(), f\"lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "        \n",
    "                \n",
    "\n",
    "create_network(load=False, loadfile = \"lr0.001_t1_4_1_15h.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a748d9-0325-4be6-9344-20940f43a2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
