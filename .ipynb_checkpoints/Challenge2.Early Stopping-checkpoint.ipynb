{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "689fa29b",
   "metadata": {},
   "source": [
    "# Key Difference Between `torch.tensor` and `np.ndarray ` in Data Type\n",
    "\n",
    "***Using Early Stopping by `state_dict()` under `torch.autograd.no_grad()`***\n",
    "**Woojeong Kim** *8/10/2024*\n",
    "\n",
    "- **Storage of Data**: Both `torch.tensor` and `np.ndarray` store n-dimensional matrices, but `torch.tensor` also stores the computational graph that leads to the associated n-dimensional matrix.\n",
    "  \n",
    "- **Computational Graph**: This graph in `torch.tensor` allows PyTorch to automatically compute gradients during backpropagation, which is crucial for training neural networks using gradient descent.\n",
    "\n",
    "- **Interchangeability**: If you are only performing mathematical operations without the need for gradient computation, `np.ndarray` and `torch.tensor` can be used interchangeably. However, when gradient computation is involved, `torch.tensor` is necessary.\n",
    "\n",
    "- **Detaching the Graph**: When converting a `torch.tensor` to `np.ndarray`, you must detach the tensor from its computational graph using the `detach()` method to avoid errors and unnecessary computational overhead.\n",
    "\n",
    "### Example:\n",
    "Here's a simple illustration:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create a tensor with requires_grad=True to track operations for gradient computation\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform a simple operation\n",
    "y = x * 2\n",
    "\n",
    "# Convert to np.ndarray without gradient tracking\n",
    "y_np = y.detach().numpy()\n",
    "\n",
    "print(y_np)\n",
    "```\n",
    "\n",
    "In this example, `y` is a `torch.tensor` that stores both the numeric values and the computational graph. By detaching it using `detach()`, we strip away the computational graph, converting it to a `np.ndarray` for further non-gradient-based operations.\n",
    "\n",
    "Also, as the below example, in the end part of the epoch for Neural Network training with backward step and optimizer, we can apply the Early Stopping by saving the model parameters without gradient computing as using `torch.autograd.no_grad()`. Under this swich-off for the recording of gradient computing, the data type of the loss value can be transformed into np array for using with comparison inequality in if statement.\n",
    "\n",
    "```python\n",
    "\n",
    "def training_function(net, iterations, learning_rate, ...):\n",
    "    \n",
    "    for epoch in range(1, iterations+1):\n",
    "        \n",
    "        ###Training steps\n",
    "        # Resetting gradients to zero\n",
    "        net.optimizer.zero_grad()\n",
    "            \n",
    "        #Loss based on Initial Condition\n",
    "        mse_BC = ...\n",
    "            \n",
    "        #Combine all Loss functions\n",
    "        loss = mse_BC + ... + mse_NS\n",
    "            \n",
    "        loss.backward()\n",
    "            \n",
    "        net.optimizer.step()\n",
    "            \n",
    "        #Print Loss every 100 Epochs\n",
    "        with torch.autograd.no_grad():\n",
    "                \n",
    "            if epoch%100 == 0:\n",
    "                #In the if statement, we also strict lower bound since early stopping is used to block over-fitting.\n",
    "                if loss.cpu().detach().numpy() < 10**(-1) and loss.cpu().detach().numpy() >= 10**(-2):\n",
    "                    torch.save(net.state_dict(), f\"EarlyStopping2nd_lr{get_lr(net.optimizer)}_t{final_time}.pt\")\n",
    "                    print('\\n  *Saved ; Early Stopping for the latest NS PDE Loss of 2nd decimal place\\n')\n",
    "                    ...\n",
    "\n",
    "```\n",
    "This distinction is crucial when transitioning between PyTorch and other libraries like NumPy, especially in deep learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a7a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
