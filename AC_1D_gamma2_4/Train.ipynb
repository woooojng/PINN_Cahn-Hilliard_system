{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4be94-868d-4299-9e65-5251b94fce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of Today :  4  / 3 \n",
      "Hour :  18\n",
      "Training PDE\n",
      "\n",
      "\n",
      "Training Pass 1\n",
      "\n",
      "\n",
      "Current End Time: 1 Current Learning Rate:  0.001\n",
      "\n",
      "Iteration: 1 \tTotal Loss: tensor(28.2044)\n",
      "\tIC Loss:  1.2486037015914917 \t BC u Loss:  0.024860374629497528 \t BC u_x Loss:  0.0029087841976433992 \n",
      "AC PDE Loss:  2.447028398513794 \tadaptive PDE Loss:  24.48102569580078\n",
      "\n",
      "Iteration: 500 \tTotal Loss: tensor(5.4927)\n",
      "\tIC Loss:  2.29304575920105 \t BC u Loss:  0.2059677541255951 \t BC u_x Loss:  0.0020739114843308926 \n",
      "AC PDE Loss:  0.27198609709739685 \tadaptive PDE Loss:  2.719658613204956\n",
      "\n",
      "Iteration: 1000 \tTotal Loss: tensor(4.9566)\n",
      "\tIC Loss:  2.19362211227417 \t BC u Loss:  0.141810342669487 \t BC u_x Loss:  0.0009309456218034029 \n",
      "AC PDE Loss:  0.23802246153354645 \tadaptive PDE Loss:  2.3822035789489746\n",
      "\n",
      "Iteration: 1500 \tTotal Loss: tensor(3.9653)\n",
      "\tIC Loss:  1.7007838487625122 \t BC u Loss:  0.06540226191282272 \t BC u_x Loss:  0.003217138350009918 \n",
      "AC PDE Loss:  0.1988632082939148 \tadaptive PDE Loss:  1.9969923496246338\n",
      "\n",
      "Iteration: 2000 \tTotal Loss: tensor(3.8122)\n",
      "\tIC Loss:  1.6180068254470825 \t BC u Loss:  0.07001256197690964 \t BC u_x Loss:  0.003654286963865161 \n",
      "AC PDE Loss:  0.19871380925178528 \tadaptive PDE Loss:  1.9218482971191406\n",
      "\n",
      "Iteration: 2500 \tTotal Loss: tensor(3.8577)\n",
      "\tIC Loss:  1.5206965208053589 \t BC u Loss:  0.07347837090492249 \t BC u_x Loss:  0.0038004706148058176 \n",
      "AC PDE Loss:  0.1991589367389679 \tadaptive PDE Loss:  2.0605766773223877\n",
      "\n",
      "Iteration: 3000 \tTotal Loss: tensor(3.6302)\n",
      "\tIC Loss:  1.4207504987716675 \t BC u Loss:  0.0797685906291008 \t BC u_x Loss:  0.00022586580598726869 \n",
      "AC PDE Loss:  0.21036271750926971 \tadaptive PDE Loss:  1.9190888404846191\n",
      "\n",
      "Iteration: 3500 \tTotal Loss: tensor(3.5642)\n",
      "\tIC Loss:  1.3994921445846558 \t BC u Loss:  0.0866752564907074 \t BC u_x Loss:  0.001269316184334457 \n",
      "AC PDE Loss:  0.17966507375240326 \tadaptive PDE Loss:  1.897062063217163\n",
      "\n",
      "Iteration: 4000 \tTotal Loss: tensor(3.5134)\n",
      "\tIC Loss:  1.2953336238861084 \t BC u Loss:  0.08355726301670074 \t BC u_x Loss:  0.0032859565690159798 \n",
      "AC PDE Loss:  0.18855541944503784 \tadaptive PDE Loss:  1.9426219463348389\n",
      "\n",
      "Iteration: 4500 \tTotal Loss: tensor(3.2650)\n",
      "\tIC Loss:  1.2999955415725708 \t BC u Loss:  0.06316131353378296 \t BC u_x Loss:  0.017491228878498077 \n",
      "AC PDE Loss:  0.1885443776845932 \tadaptive PDE Loss:  1.6958069801330566\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from scipy.stats import qmc\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from Building_Net import Net\n",
    "from Conditions import lossIC, lossBdry,lossNSpde, lossNSpde_rank\n",
    "\n",
    "currentDateTime = datetime.now()\n",
    "print(\"Date of Today : \", currentDateTime.month, \" /\", currentDateTime.day, \"\\nHour : \", currentDateTime.hour) \n",
    "ctime = f\"{currentDateTime.month}_{currentDateTime.day}_{currentDateTime.hour}h\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_network(load, loadfile):\n",
    "    \n",
    "    net = Net()\n",
    "    net = net.to(device)\n",
    "    epsilon = []\n",
    "    #Attempt to load the saved pt. file\n",
    "    if load == True:\n",
    "        try:\n",
    "            net.load_state_dict(torch.load(loadfile, map_location=torch.device(device)))\n",
    "        except:\n",
    "            print(\"\\nLoading file was failed\\n\")\n",
    "        else:\n",
    "            print(\"\\nLoading file was completed\\n\")\n",
    "    \n",
    "    print('Training PDE')\n",
    "    start = time.time() #initialize tracking computational time\n",
    "    \n",
    "    partial_time_set = [0, 0, 0] #initialize time recording list with number of learnning rates on whole domain training\n",
    "    \n",
    "    for i in range(len(partial_time_set)):\n",
    "        if i == 0:\n",
    "            #First loop uses progressively increasing time intervals\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            time_slices = np.array([1])\n",
    "            iterations = 30000 #iterations for each learning rate\n",
    "            learning_rate = 10**-3\n",
    "        elif i == 1:\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            #time_slices = [time_slices[-1]]\n",
    "            time_slices = [time_slices[-1]]\n",
    "            iterations = 30000 #iterations for each learning rate\n",
    "            learning_rate = 10**-4\n",
    "        elif i == 2:\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            time_slices = [time_slices[-1]]\n",
    "            iterations = 0 #iterations for each learning rate\n",
    "            learning_rate = 10**-4\n",
    "        \n",
    "        training_loop(net, time_slices, iterations, learning_rate, record_loss = 100, print_loss = 500, epsilon = epsilon)\n",
    "        torch.save(net.state_dict(), f\"{ctime}_Training_{i+1}.pt\")\n",
    "        partial_time_set[i] = time.time()\n",
    "        np.savetxt(f\"{ctime}epsilon_{i}.txt\", epsilon)\n",
    "\n",
    "    print(\"Total Time:\\t\", partial_time_set[-1]-start, '\\nPass 1 Time:\\t', partial_time_set[0]-start, \n",
    "          '\\nPass 2 Time:\\t', partial_time_set[1]-partial_time_set[0], '\\nPass 3 Time:\\t', partial_time_set[2]-partial_time_set[1])\n",
    "   \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def training_loop(net, time_slices, iterations, learning_rate, record_loss, print_loss, epsilon):\n",
    "    \n",
    "    # Domain boundary values\n",
    "    x_l = net.x1_l\n",
    "    x_u = net.x1_u\n",
    "    \n",
    "    #time starts at 0, ends at upper bouund updated in time_slices\n",
    "    t_l = 0\n",
    "\n",
    "    #numbers of sampling collocation points on each part\n",
    "    IC_collocation = int(200)\n",
    "    BC_collocation = int(200)\n",
    "    pde_collocation = int(2000)\n",
    "\n",
    "    #sampler setting for LatinHypercube sampling\n",
    "    IC_lh_sampler = qmc.LatinHypercube(d=1)\n",
    "    BC_lh_sampler = qmc.LatinHypercube(d=1)\n",
    "    PDE_lh_sampler = qmc.LatinHypercube(d=2)\n",
    "    PDE_lh_sampler2 = qmc.LatinHypercube(d=2)\n",
    "    #update the learning rate as defined\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    #Iterate over time slices\n",
    "    for final_time in time_slices:\n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"\\n\\nCurrent End Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        epoch = 0\n",
    "        for epoch in range(1, iterations):\n",
    "\n",
    "            # initialize gradients to zero\n",
    "            net.optimizer.zero_grad()\n",
    "\n",
    "            ##Define input points with LatinHypercube sampling\n",
    "            x_IC = x_l + (x_u-x_l) *IC_lh_sampler.random(n=IC_collocation).reshape(IC_collocation,1)\n",
    "            t_IC = np.random.uniform(low=t_l, high=t_l, size=(IC_collocation,1))\n",
    "            \n",
    "            input_x_IC = Variable(torch.from_numpy(x_IC).float(), requires_grad=True).to(device)\n",
    "            input_t_IC = Variable(torch.from_numpy(t_IC).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            t_BC = t_l + (final_time-t_l) *BC_lh_sampler.random(n=BC_collocation).reshape(BC_collocation,1)\n",
    "    \n",
    "            input_t_BC = Variable(torch.from_numpy(t_BC).float(), requires_grad=True).to(device)\n",
    "    \n",
    "            x_domain = x_l + (x_u-x_l) *PDE_lh_sampler.random(n=pde_collocation)[:,0].reshape(pde_collocation,1)\n",
    "            t_domain = t_l + (final_time-t_l) *PDE_lh_sampler.random(n=pde_collocation)[:,1].reshape(pde_collocation,1)\n",
    "\n",
    "            input_x_domain = Variable(torch.from_numpy(x_domain).float(), requires_grad=True).to(device)\n",
    "            input_t_domain = Variable(torch.from_numpy(t_domain).float(), requires_grad=True).to(device)\n",
    "\n",
    "            x_domain_r = x_l + (x_u-x_l) *PDE_lh_sampler2.random(n=pde_collocation)[:,0].reshape(pde_collocation,1)\n",
    "            t_domain_r = t_l + (final_time-t_l) *PDE_lh_sampler2.random(n=pde_collocation)[:,1].reshape(pde_collocation,1)\n",
    "\n",
    "            input_x_domain_r = Variable(torch.from_numpy(x_domain_r).float(), requires_grad=True).to(device)\n",
    "            input_t_domain_r = Variable(torch.from_numpy(t_domain_r).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            #Take additive appaptive sampling with 500 highest loss points\n",
    "            PDEloss_tensor= lossNSpde_rank(net, input_x_domain_r, input_t_domain_r)     \n",
    "            \n",
    "            sorted_tensor, indices = torch.sort(PDEloss_tensor.view(-1), descending=True)\n",
    "            sorted_tensor = sorted_tensor.view(-1, 1)\n",
    "            \n",
    "            max_stad = sorted_tensor[int(pde_collocation/10)-1,0]\n",
    "            \n",
    "            PDEloss_picked = torch.where(PDEloss_tensor>=max_stad, PDEloss_tensor, 0)\n",
    "            #x_domain_r = torch.where(PDEloss_tensor==PDEloss_picked, input_x_domain_r, 0)\n",
    "            #t_domain_r = torch.where(PDEloss_tensor==PDEloss_picked, input_t_domain_r, 0)\n",
    "            \n",
    "            PDEloss_adaptive = lossNSpde(net, input_x_domain_r, input_t_domain_r)*10 #(torch.sum(PDEloss_picked**2)/(int(pde_collocation/10)))**.5\n",
    "\n",
    "            \n",
    "            #Loss computation\n",
    "            u_IC_loss = lossIC(net, input_x_IC, input_t_IC) #, u_IC_loss_mesh\n",
    "            mse_IC = u_IC_loss\n",
    "            \n",
    "            #Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "            mse_BC_u, mse_BC_u_x = lossBdry(net, input_t_BC) \n",
    "            mse_BC = mse_BC_u+ mse_BC_u_x \n",
    "            \n",
    "            #Loss based on PDE\n",
    "            AC_mse= lossNSpde(net, input_x_domain, input_t_domain) \n",
    "            mse_PDE = AC_mse + PDEloss_adaptive\n",
    "        \n",
    "            loss =  (mse_BC + mse_IC + mse_PDE )\n",
    "        \n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            def closure():\n",
    "                return loss\n",
    "            \n",
    "            #Make Iterative Step\n",
    "            net.optimizer.step() #net.optimizer.step(closure)\n",
    "            \n",
    "            \n",
    "            # Gradient Norm Clipping\n",
    "            #torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm = 1000, error_if_nonfinite=False) #note max norm is more art than science\n",
    "            \n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch == 1 or epoch%print_loss == 0:\n",
    "                    print(\"\\nIteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    #,\"\\tadaptive IC Loss2: \", mse_IC_a2.item() , \"\\tadaptive CH PDE Loss2: \", CH_a2.item()\n",
    "                    print(\"\\tIC Loss: \", mse_IC.item(),\n",
    "                          \"\\t BC u Loss: \", mse_BC_u.item(), \"\\t BC u_x Loss: \", mse_BC_u_x.item(),\n",
    "                          \"\\nAC PDE Loss: \", AC_mse.item(), \"\\tadaptive PDE Loss: \", PDEloss_adaptive.item()\n",
    "                         ) \n",
    "                if epoch%6000 == 0:\n",
    "                    np.savetxt(f\"{ctime}epsilon.txt\", epsilon)\n",
    "                    torch.save(net.state_dict(), f\"lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "        \n",
    "                \n",
    "\n",
    "create_network(load=False, loadfile = \"lr0.001_t1_4_1_15h.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a748d9-0325-4be6-9344-20940f43a2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
