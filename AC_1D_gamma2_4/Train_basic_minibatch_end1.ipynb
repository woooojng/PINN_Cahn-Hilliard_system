{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd4be94-868d-4299-9e65-5251b94fce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of Today :  4  / 11 \n",
      "Hour :  10\n",
      "Training PDE\n",
      "\n",
      "\n",
      "Training Pass 1\n",
      "\n",
      "\n",
      "Current End Time: 1 Current Learning Rate:  0.001\n",
      "\n",
      "Iteration: 2 \tTotal Loss: tensor(4.5496)\n",
      "\tIC Loss:  0.0022433989215642214 \t BC u Loss:  0.10103770345449448 \t BC u_x Loss:  1.2530611753463745 \n",
      "AC PDE Loss:  0.08555881679058075 \tadaptive PDE Loss:  0.8665862679481506\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 4 \tTotal Loss: tensor(1.7973)\n",
      "\tIC Loss:  0.00016959242930170149 \t BC u Loss:  0.09744101762771606 \t BC u_x Loss:  0.9832364916801453 \n",
      "AC PDE Loss:  0.04803754761815071 \tadaptive PDE Loss:  0.498943567276001\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 6 \tTotal Loss: tensor(1.3650)\n",
      "\tIC Loss:  6.810888589825481e-05 \t BC u Loss:  0.11805014312267303 \t BC u_x Loss:  0.6852134466171265 \n",
      "AC PDE Loss:  0.044697459787130356 \tadaptive PDE Loss:  0.44895830750465393\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 8 \tTotal Loss: tensor(1.0773)\n",
      "\tIC Loss:  2.9974760764162056e-05 \t BC u Loss:  0.11751241981983185 \t BC u_x Loss:  0.47826701402664185 \n",
      "AC PDE Loss:  0.04115650802850723 \tadaptive PDE Loss:  0.41041451692581177\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 10 \tTotal Loss: tensor(0.8306)\n",
      "\tIC Loss:  2.9170218112994917e-05 \t BC u Loss:  0.10954505950212479 \t BC u_x Loss:  0.2912366986274719 \n",
      "AC PDE Loss:  0.036525532603263855 \tadaptive PDE Loss:  0.3640982508659363\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 12 \tTotal Loss: tensor(0.7332)\n",
      "\tIC Loss:  0.00015419803094118834 \t BC u Loss:  0.07237260043621063 \t BC u_x Loss:  0.14074797928333282 \n",
      "AC PDE Loss:  0.03377259895205498 \tadaptive PDE Loss:  0.3320832848548889\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 14 \tTotal Loss: tensor(0.6408)\n",
      "\tIC Loss:  0.00014963274588808417 \t BC u Loss:  0.08608163148164749 \t BC u_x Loss:  0.09606273472309113 \n",
      "AC PDE Loss:  0.028697088360786438 \tadaptive PDE Loss:  0.28035682439804077\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 16 \tTotal Loss: tensor(0.3554)\n",
      "\tIC Loss:  7.950818144308869e-06 \t BC u Loss:  0.0553574375808239 \t BC u_x Loss:  0.05165648087859154 \n",
      "AC PDE Loss:  0.022002780809998512 \tadaptive PDE Loss:  0.21844995021820068\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 18 \tTotal Loss: tensor(0.6935)\n",
      "\tIC Loss:  0.0003077915171161294 \t BC u Loss:  0.041356541216373444 \t BC u_x Loss:  0.15508711338043213 \n",
      "AC PDE Loss:  0.01749703846871853 \tadaptive PDE Loss:  0.17171891033649445\n",
      "\n",
      "Iteration: 20 \tTotal Loss: tensor(2.1581)\n",
      "\tIC Loss:  0.0018546616192907095 \t BC u Loss:  0.053460609167814255 \t BC u_x Loss:  0.10558965057134628 \n",
      "AC PDE Loss:  0.012135429307818413 \tadaptive PDE Loss:  0.13227811455726624\n",
      "\n",
      "Iteration: 22 \tTotal Loss: tensor(0.2530)\n",
      "\tIC Loss:  1.0766007108031772e-05 \t BC u Loss:  0.032053932547569275 \t BC u_x Loss:  0.10329771041870117 \n",
      "AC PDE Loss:  0.009555727243423462 \tadaptive PDE Loss:  0.09730003029108047\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 24 \tTotal Loss: tensor(0.4576)\n",
      "\tIC Loss:  0.00021672245929948986 \t BC u Loss:  0.01988573744893074 \t BC u_x Loss:  0.14836479723453522 \n",
      "AC PDE Loss:  0.007842901162803173 \tadaptive PDE Loss:  0.0647466704249382\n",
      "\n",
      "Iteration: 26 \tTotal Loss: tensor(0.8715)\n",
      "\tIC Loss:  0.0006582780624739826 \t BC u Loss:  0.017354605719447136 \t BC u_x Loss:  0.12534990906715393 \n",
      "AC PDE Loss:  0.0061350492760539055 \tadaptive PDE Loss:  0.06438612937927246\n",
      "\n",
      "Iteration: 28 \tTotal Loss: tensor(0.1537)\n",
      "\tIC Loss:  9.299767953052651e-06 \t BC u Loss:  0.017369307577610016 \t BC u_x Loss:  0.07564380019903183 \n",
      "AC PDE Loss:  0.004369023721665144 \tadaptive PDE Loss:  0.04698580130934715\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 30 \tTotal Loss: tensor(0.7064)\n",
      "\tIC Loss:  0.0006182087236084044 \t BC u Loss:  0.008706163614988327 \t BC u_x Loss:  0.03471028804779053 \n",
      "AC PDE Loss:  0.004257513210177422 \tadaptive PDE Loss:  0.040500130504369736\n",
      "\n",
      "Iteration: 32 \tTotal Loss: tensor(0.2684)\n",
      "\tIC Loss:  0.00011398408969398588 \t BC u Loss:  0.007102193310856819 \t BC u_x Loss:  0.11623445153236389 \n",
      "AC PDE Loss:  0.0030084052123129368 \tadaptive PDE Loss:  0.02810787782073021\n",
      "\n",
      "Iteration: 34 \tTotal Loss: tensor(0.6603)\n",
      "\tIC Loss:  0.000549571355804801 \t BC u Loss:  0.015402168966829777 \t BC u_x Loss:  0.05976910889148712 \n",
      "AC PDE Loss:  0.003391919657588005 \tadaptive PDE Loss:  0.03215303272008896\n",
      "\n",
      "Iteration: 36 \tTotal Loss: tensor(0.3916)\n",
      "\tIC Loss:  8.452501060673967e-05 \t BC u Loss:  0.005402462091296911 \t BC u_x Loss:  0.28051069378852844 \n",
      "AC PDE Loss:  0.0020688220392912626 \tadaptive PDE Loss:  0.019075440242886543\n",
      "\n",
      "Iteration: 38 \tTotal Loss: tensor(0.6994)\n",
      "\tIC Loss:  0.0002542413421906531 \t BC u Loss:  0.0024074509274214506 \t BC u_x Loss:  0.41967272758483887 \n",
      "AC PDE Loss:  0.0020505054853856564 \tadaptive PDE Loss:  0.02104254812002182\n",
      "\n",
      "Iteration: 40 \tTotal Loss: tensor(0.8413)\n",
      "\tIC Loss:  0.0006213317392393947 \t BC u Loss:  0.004181521479040384 \t BC u_x Loss:  0.1932881474494934 \n",
      "AC PDE Loss:  0.0021846562158316374 \tadaptive PDE Loss:  0.020278610289096832\n",
      "\n",
      "Iteration: 42 \tTotal Loss: tensor(0.2004)\n",
      "\tIC Loss:  8.03075308795087e-05 \t BC u Loss:  0.004329277202486992 \t BC u_x Loss:  0.10149115324020386 \n",
      "AC PDE Loss:  0.0017340061021968722 \tadaptive PDE Loss:  0.012528777122497559\n",
      "\n",
      "Iteration: 44 \tTotal Loss: tensor(0.2455)\n",
      "\tIC Loss:  9.26613574847579e-05 \t BC u Loss:  0.004711547400802374 \t BC u_x Loss:  0.1348697990179062 \n",
      "AC PDE Loss:  0.0013908109394833446 \tadaptive PDE Loss:  0.011869890615344048\n",
      "\n",
      "Iteration: 46 \tTotal Loss: tensor(0.3469)\n",
      "\tIC Loss:  5.092700303066522e-05 \t BC u Loss:  0.0005015069036744535 \t BC u_x Loss:  0.28524109721183777 \n",
      "AC PDE Loss:  0.0010717620607465506 \tadaptive PDE Loss:  0.009198195300996304\n",
      "\n",
      "Iteration: 48 \tTotal Loss: tensor(0.5326)\n",
      "\tIC Loss:  3.356613888172433e-05 \t BC u Loss:  0.0009389864862896502 \t BC u_x Loss:  0.48867446184158325 \n",
      "AC PDE Loss:  0.0009341256227344275 \tadaptive PDE Loss:  0.00847444124519825\n",
      "\n",
      "Iteration: 50 \tTotal Loss: tensor(1.0243)\n",
      "\tIC Loss:  1.3572337138612056e-06 \t BC u Loss:  0.002376232063397765 \t BC u_x Loss:  1.0087192058563232 \n",
      "AC PDE Loss:  0.0011673211120069027 \tadaptive PDE Loss:  0.010729878209531307\n",
      "\n",
      "Iteration: 52 \tTotal Loss: tensor(2.1931)\n",
      "\tIC Loss:  0.0016667761374264956 \t BC u Loss:  0.007509859744459391 \t BC u_x Loss:  0.49286267161369324 \n",
      "AC PDE Loss:  0.0029643692541867495 \tadaptive PDE Loss:  0.023008350282907486\n",
      "\n",
      "Iteration: 54 \tTotal Loss: tensor(0.5368)\n",
      "\tIC Loss:  5.08723087477847e-06 \t BC u Loss:  0.0024123177863657475 \t BC u_x Loss:  0.5205375552177429 \n",
      "AC PDE Loss:  0.0010328107746317983 \tadaptive PDE Loss:  0.0076809898018836975\n",
      "\n",
      "Iteration: 56 \tTotal Loss: tensor(0.6682)\n",
      "\tIC Loss:  5.338025221135467e-05 \t BC u Loss:  0.0008314852602779865 \t BC u_x Loss:  0.6082947254180908 \n",
      "AC PDE Loss:  0.0008127774344757199 \tadaptive PDE Loss:  0.004895647056400776\n",
      "\n",
      "Iteration: 58 \tTotal Loss: tensor(1.2421)\n",
      "\tIC Loss:  4.13922498410102e-05 \t BC u Loss:  0.000408699968829751 \t BC u_x Loss:  1.1947855949401855 \n",
      "AC PDE Loss:  0.0008654098492115736 \tadaptive PDE Loss:  0.004672203678637743\n",
      "\n",
      "Iteration: 60 \tTotal Loss: tensor(1.1024)\n",
      "\tIC Loss:  8.112422801787034e-05 \t BC u Loss:  0.00035153678618371487 \t BC u_x Loss:  1.0166159868240356 \n",
      "AC PDE Loss:  0.0006021842709742486 \tadaptive PDE Loss:  0.0036815854255110025\n",
      "\n",
      "Iteration: 62 \tTotal Loss: tensor(0.8272)\n",
      "\tIC Loss:  9.492688946011185e-07 \t BC u Loss:  0.0007782770553603768 \t BC u_x Loss:  0.8221790790557861 \n",
      "AC PDE Loss:  0.0005251505062915385 \tadaptive PDE Loss:  0.0028001023456454277\n",
      "\n",
      "Iteration: 64 \tTotal Loss: tensor(0.6091)\n",
      "\tIC Loss:  2.952249087684322e-05 \t BC u Loss:  0.0003675589105114341 \t BC u_x Loss:  0.5749226212501526 \n",
      "AC PDE Loss:  0.0004601891851052642 \tadaptive PDE Loss:  0.0038465301040560007\n",
      "\n",
      "Iteration: 66 \tTotal Loss: tensor(1.2646)\n",
      "\tIC Loss:  3.3896537843247643e-06 \t BC u Loss:  0.00043606304097920656 \t BC u_x Loss:  1.2576347589492798 \n",
      "AC PDE Loss:  0.0006109990063123405 \tadaptive PDE Loss:  0.0024840086698532104\n",
      "\n",
      "Iteration: 68 \tTotal Loss: tensor(2.8244)\n",
      "\tIC Loss:  2.5905137590598315e-06 \t BC u Loss:  0.00023066964058671147 \t BC u_x Loss:  2.8191356658935547 \n",
      "AC PDE Loss:  0.0002501186972949654 \tadaptive PDE Loss:  0.002205197000876069\n",
      "\n",
      "Iteration: 70 \tTotal Loss: tensor(2.8678)\n",
      "\tIC Loss:  0.0003113228012807667 \t BC u Loss:  0.0005561879370361567 \t BC u_x Loss:  2.5487358570098877 \n",
      "AC PDE Loss:  0.0007620886899530888 \tadaptive PDE Loss:  0.006376095116138458\n",
      "\n",
      "Iteration: 72 \tTotal Loss: tensor(3.2323)\n",
      "\tIC Loss:  1.813731978472788e-05 \t BC u Loss:  0.00028720125555992126 \t BC u_x Loss:  3.2119383811950684 \n",
      "AC PDE Loss:  0.0002670976100489497 \tadaptive PDE Loss:  0.0016808091895654798\n",
      "\n",
      "Iteration: 74 \tTotal Loss: tensor(2.2703)\n",
      "\tIC Loss:  0.00012564710050355643 \t BC u Loss:  0.0010103732347488403 \t BC u_x Loss:  2.140643358230591 \n",
      "AC PDE Loss:  0.0002989855711348355 \tadaptive PDE Loss:  0.0027255259919911623\n",
      "\n",
      "Iteration: 76 \tTotal Loss: tensor(2.3028)\n",
      "\tIC Loss:  1.0910431228694506e-05 \t BC u Loss:  0.0002014461933868006 \t BC u_x Loss:  2.2898926734924316 \n",
      "AC PDE Loss:  0.00033869215985760093 \tadaptive PDE Loss:  0.0015006877947598696\n",
      "\n",
      "Iteration: 78 \tTotal Loss: tensor(3.6351)\n",
      "\tIC Loss:  5.752781362389214e-07 \t BC u Loss:  0.0001288316707359627 \t BC u_x Loss:  3.6324684619903564 \n",
      "AC PDE Loss:  0.0003193019947502762 \tadaptive PDE Loss:  0.001638750545680523\n",
      "\n",
      "Iteration: 80 \tTotal Loss: tensor(1.6728)\n",
      "\tIC Loss:  5.304377191350795e-05 \t BC u Loss:  0.0001110716475523077 \t BC u_x Loss:  1.6180784702301025 \n",
      "AC PDE Loss:  0.0002393651520833373 \tadaptive PDE Loss:  0.0013752415543422103\n",
      "\n",
      "Iteration: 82 \tTotal Loss: tensor(4.4549)\n",
      "\tIC Loss:  0.00037200923543423414 \t BC u Loss:  0.00019670877372846007 \t BC u_x Loss:  4.078798770904541 \n",
      "AC PDE Loss:  0.00036217266460880637 \tadaptive PDE Loss:  0.003574013477191329\n",
      "\n",
      "Iteration: 84 \tTotal Loss: tensor(2.7008)\n",
      "\tIC Loss:  3.393676888663322e-05 \t BC u Loss:  0.0001801289472496137 \t BC u_x Loss:  2.6654303073883057 \n",
      "AC PDE Loss:  0.00022142323723528534 \tadaptive PDE Loss:  0.0009934273548424244\n",
      "\n",
      "Iteration: 86 \tTotal Loss: tensor(3.5232)\n",
      "\tIC Loss:  4.19089786873883e-07 \t BC u Loss:  4.406441348692169e-06 \t BC u_x Loss:  3.5217020511627197 \n",
      "AC PDE Loss:  0.0001514966133981943 \tadaptive PDE Loss:  0.0009278670768253505\n",
      "\n",
      "Iteration: 88 \tTotal Loss: tensor(2.9542)\n",
      "\tIC Loss:  0.00022447477385867387 \t BC u Loss:  0.00022673074272461236 \t BC u_x Loss:  2.7279562950134277 \n",
      "AC PDE Loss:  0.0002423787664156407 \tadaptive PDE Loss:  0.00132539845071733\n",
      "\n",
      "Iteration: 90 \tTotal Loss: tensor(1.7237)\n",
      "\tIC Loss:  0.000141912154504098 \t BC u Loss:  4.744884790852666e-06 \t BC u_x Loss:  1.579878807067871 \n",
      "AC PDE Loss:  0.00020465889247134328 \tadaptive PDE Loss:  0.001743164611980319\n",
      "\n",
      "Iteration: 92 \tTotal Loss: tensor(0.9193)\n",
      "\tIC Loss:  0.00022240387625060976 \t BC u Loss:  0.00022307824110612273 \t BC u_x Loss:  0.693610429763794 \n",
      "AC PDE Loss:  0.0003171113785356283 \tadaptive PDE Loss:  0.0027299136854708195\n",
      "\n",
      "Iteration: 94 \tTotal Loss: tensor(4.2207)\n",
      "\tIC Loss:  0.0008758305921219289 \t BC u Loss:  0.00013878241588827223 \t BC u_x Loss:  3.336665391921997 \n",
      "AC PDE Loss:  0.0008300362387672067 \tadaptive PDE Loss:  0.007244820706546307\n",
      "\n",
      "Iteration: 96 \tTotal Loss: tensor(2.0827)\n",
      "\tIC Loss:  1.4719574210175779e-05 \t BC u Loss:  0.00010150555317522958 \t BC u_x Loss:  2.0668697357177734 \n",
      "AC PDE Loss:  0.00013752648374065757 \tadaptive PDE Loss:  0.0008370023570023477\n",
      "\n",
      "Iteration: 98 \tTotal Loss: tensor(2.4339)\n",
      "\tIC Loss:  5.618532668449916e-05 \t BC u Loss:  0.0006528860540129244 \t BC u_x Loss:  2.3760616779327393 \n",
      "AC PDE Loss:  0.00012403904111124575 \tadaptive PDE Loss:  0.0008562233997508883\n",
      "\n",
      "\n",
      "Training Pass 2\n",
      "\n",
      "\n",
      "Current End Time: 1 Current Learning Rate:  0.0001\n",
      "\n",
      "\n",
      "Training Pass 3\n",
      "\n",
      "\n",
      "Current End Time: 1 Current Learning Rate:  0.0001\n",
      "Total Time:\t 132564.03235006332 \n",
      "Pass 1 Time:\t 132564.01467084885 \n",
      "Pass 2 Time:\t 0.012543201446533203 \n",
      "Pass 3 Time:\t 0.005136013031005859\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from scipy.stats import qmc\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from Building_Net import Net\n",
    "from Conditions import lossIC, lossBdry,lossNSpde, lossNSpde_rank\n",
    "\n",
    "currentDateTime = datetime.now()\n",
    "print(\"Date of Today : \", currentDateTime.month, \" /\", currentDateTime.day, \"\\nHour : \", currentDateTime.hour) \n",
    "ctime = f\"{currentDateTime.month}_{currentDateTime.day}_{currentDateTime.hour}h\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_network(load, loadfile):\n",
    "    \n",
    "    net = Net()\n",
    "    net = net.to(device)\n",
    "    epsilon = []\n",
    "    #Attempt to load the saved pt. file\n",
    "    if load == True:\n",
    "        try:\n",
    "            net.load_state_dict(torch.load(loadfile, map_location=torch.device(device)))\n",
    "        except:\n",
    "            print(\"\\nLoading file was failed\\n\")\n",
    "        else:\n",
    "            print(\"\\nLoading file was completed\\n\")\n",
    "    \n",
    "    print('Training PDE')\n",
    "    start = time.time() #initialize tracking computational time\n",
    "    \n",
    "    partial_time_set = [0, 0, 0] #initialize time recording list with number of learnning rates on whole domain training\n",
    "    \n",
    "    for i in range(len(partial_time_set)):\n",
    "        if i == 0:\n",
    "            #First loop uses progressively increasing time intervals\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            time_slices = np.array([1])\n",
    "            iterations = 100 #iterations for each learning rate\n",
    "            learning_rate = 10**-3\n",
    "        elif i == 1:\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            #time_slices = [time_slices[-1]]\n",
    "            time_slices = [time_slices[-1]]\n",
    "            iterations = 0 #iterations for each learning rate\n",
    "            learning_rate = 10**-4\n",
    "        elif i == 2:\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            time_slices = [time_slices[-1]]\n",
    "            iterations = 0 #iterations for each learning rate\n",
    "            learning_rate = 10**-4\n",
    "        \n",
    "        training_loop(net, time_slices, iterations, learning_rate, record_loss = 100, print_loss = 500, epsilon = epsilon)\n",
    "        torch.save(net.state_dict(), f\"{ctime}_Training_{i+1}.pt\")\n",
    "        partial_time_set[i] = time.time()\n",
    "        np.savetxt(f\"{ctime}epsilon_{i}.txt\", epsilon)\n",
    "\n",
    "    print(\"Total Time:\\t\", partial_time_set[-1]-start, '\\nPass 1 Time:\\t', partial_time_set[0]-start, \n",
    "          '\\nPass 2 Time:\\t', partial_time_set[1]-partial_time_set[0], '\\nPass 3 Time:\\t', partial_time_set[2]-partial_time_set[1])\n",
    "   \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def exp_time_sample(collocation_pt_number, t_l, t_u, r):\n",
    "    t_uni = np.random.uniform(0,1, size=(collocation_pt_number, 1))\n",
    "    \n",
    "    t = -np.log(1-t_uni+t_uni*np.exp(-r*(t_u-t_l)))/r\n",
    "    return t\n",
    "    \n",
    "def training_loop(net, time_slices, iterations, learning_rate, record_loss, print_loss, epsilon):\n",
    "    min_loss = 5\n",
    "    \n",
    "    # Domain boundary values\n",
    "    x_l = net.x1_l\n",
    "    x_u = net.x1_u\n",
    "    \n",
    "    #time starts at 0, ends at upper bouund updated in time_slices\n",
    "    t_l = 0\n",
    "\n",
    "    #numbers of sampling collocation points on each part\n",
    "    IC_collocation = int(512)\n",
    "    BC_collocation = int(200)\n",
    "    pde_collocation = int(20000)\n",
    "\n",
    "    #sampler setting for LatinHypercube sampling\n",
    "    IC_lh_sampler = qmc.LatinHypercube(d=1)\n",
    "    BC_lh_sampler = qmc.LatinHypercube(d=1)\n",
    "    PDE_lh_sampler = qmc.LatinHypercube(d=2)\n",
    "    PDE_lh_sampler2 = qmc.LatinHypercube(d=2)\n",
    "    #update the learning rate as defined\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    #Iterate over time slices\n",
    "    for final_time in time_slices:\n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"\\n\\nCurrent End Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        epoch = 0\n",
    "        for epoch in range(1, iterations):\n",
    "\n",
    "            \n",
    "            ##Define input points with LatinHypercube sampling\n",
    "            x_IC = x_l + (x_u-x_l) *IC_lh_sampler.random(n=IC_collocation).reshape(IC_collocation,1)\n",
    "            t_IC = np.random.uniform(low=t_l, high=t_l, size=(IC_collocation,1))\n",
    "            \n",
    "            input_x_IC = Variable(torch.from_numpy(x_IC).float(), requires_grad=True).to(device)\n",
    "            input_t_IC = Variable(torch.from_numpy(t_IC).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            #t_BC = t_l + (final_time-t_l) *BC_lh_sampler.random(n=BC_collocation).reshape(BC_collocation,1)\n",
    "            t_BC_exp = exp_time_sample(BC_collocation, t_l, final_time, r=10)\n",
    "            \n",
    "            #input_t_BC = Variable(torch.from_numpy(t_BC).float(), requires_grad=True).to(device)\n",
    "            input_t_BC = Variable(torch.from_numpy(t_BC_exp).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            x_domain = x_l + (x_u-x_l) *PDE_lh_sampler.random(n=pde_collocation)[:,0].reshape(pde_collocation,1)\n",
    "            #t_domain = t_l + (final_time-t_l) *PDE_lh_sampler.random(n=pde_collocation)[:,1].reshape(pde_collocation,1)\n",
    "            t_domain_exp = exp_time_sample(pde_collocation, t_l, final_time, r=10)\n",
    "\n",
    "            input_x_domain = Variable(torch.from_numpy(x_domain).float(), requires_grad=True).to(device)\n",
    "            #input_t_domain = Variable(torch.from_numpy(t_domain).float(), requires_grad=True).to(device)\n",
    "            input_t_domain = Variable(torch.from_numpy(t_domain_exp).float(), requires_grad=True).to(device)\n",
    "\n",
    "            x_domain_r = x_l + (x_u-x_l) *PDE_lh_sampler2.random(n=pde_collocation)[:,0].reshape(pde_collocation,1)\n",
    "            #t_domain_r = t_l + (final_time-t_l) *PDE_lh_sampler2.random(n=pde_collocation)[:,1].reshape(pde_collocation,1)\n",
    "            t_domain_r_exp = exp_time_sample(pde_collocation, t_l, final_time, r=10)\n",
    "\n",
    "            input_x_domain_r = Variable(torch.from_numpy(x_domain_r).float(), requires_grad=True).to(device)\n",
    "            #input_t_domain_r = Variable(torch.from_numpy(t_domain_r).float(), requires_grad=True).to(device)\n",
    "            input_t_domain_r = Variable(torch.from_numpy(t_domain_r_exp).float(), requires_grad=True).to(device)\n",
    "\n",
    "            for i in list(range(0, int(pde_collocation//32)+1)):\n",
    "                # initialize gradients to zero\n",
    "                net.optimizer.zero_grad()\n",
    "                #Take additive appaptive sampling with 500 highest loss points\n",
    "                PDEloss_tensor= lossNSpde_rank(net, input_x_domain_r, input_t_domain_r)     \n",
    "                \n",
    "                sorted_tensor, indices = torch.sort(PDEloss_tensor.view(-1), descending=True)\n",
    "                sorted_tensor = sorted_tensor.view(-1, 1)\n",
    "                \n",
    "                max_stad = sorted_tensor[int(pde_collocation/10)-1,0]\n",
    "                \n",
    "                PDEloss_picked = torch.where(PDEloss_tensor>=max_stad, PDEloss_tensor, 0)\n",
    "                #x_domain_r = torch.where(PDEloss_tensor==PDEloss_picked, input_x_domain_r, 0)\n",
    "                #t_domain_r = torch.where(PDEloss_tensor==PDEloss_picked, input_t_domain_r, 0)\n",
    "                \n",
    "                PDEloss_adaptive = lossNSpde(net, input_x_domain_r, input_t_domain_r)*10 #(torch.sum(PDEloss_picked**2)/(int(pde_collocation/10)))**.5\n",
    "    \n",
    "                \n",
    "                #Loss computation\n",
    "                u_IC_loss = lossIC(net, input_x_IC, input_t_IC) #, u_IC_loss_mesh\n",
    "                mse_IC = u_IC_loss\n",
    "                \n",
    "                #Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "                mse_BC_u, mse_BC_u_x = lossBdry(net, input_t_BC) \n",
    "                mse_BC = mse_BC_u+ mse_BC_u_x \n",
    "                \n",
    "                #Loss based on PDE\n",
    "                AC_mse= lossNSpde(net, input_x_domain, input_t_domain) \n",
    "                mse_PDE = AC_mse + PDEloss_adaptive\n",
    "            \n",
    "                loss =  (mse_BC + 1000 * mse_IC + mse_PDE )\n",
    "            \n",
    "                loss.backward()\n",
    "                \n",
    "                \n",
    "                def closure():\n",
    "                    return loss\n",
    "                \n",
    "                #Make Iterative Step\n",
    "                net.optimizer.step() #net.optimizer.step(closure)\n",
    "            \n",
    "            \n",
    "            # Gradient Norm Clipping\n",
    "            #torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm = 1000, error_if_nonfinite=False) #note max norm is more art than science\n",
    "            \n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch% 2 == 0:#print_loss epoch == 1 or \n",
    "                    print(\"\\nIteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    #,\"\\tadaptive IC Loss2: \", mse_IC_a2.item() , \"\\tadaptive CH PDE Loss2: \", CH_a2.item()\n",
    "                    print(\"\\tIC Loss: \", mse_IC.item(),\n",
    "                          \"\\t BC u Loss: \", mse_BC_u.item(), \"\\t BC u_x Loss: \", mse_BC_u_x.item(),\n",
    "                          \"\\nAC PDE Loss: \", AC_mse.item(), \"\\tadaptive PDE Loss: \", PDEloss_adaptive.item()\n",
    "                         ) \n",
    "                    if loss.cpu().detach().numpy() < min_loss:\n",
    "                        min_loss = loss.cpu().detach().numpy()\n",
    "                        torch.save(net.state_dict(), f\"ES_min_loss_lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                        print('  *Saved ; Early Stopping for the Minimal Total Loss\\n')\n",
    "                if epoch%6 == 0:\n",
    "                    np.savetxt(f\"{ctime}epsilon.txt\", epsilon)\n",
    "                    torch.save(net.state_dict(), f\"lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                if epoch%2 == 0:\n",
    "                    torch.save( torch.cat([input_x_IC, input_t_IC],axis=1), 'initial.pt')\n",
    "                    torch.save( torch.cat([input_x_domain, input_t_domain],axis=1), 'domain_latin.pt')\n",
    "                    torch.save(torch.cat([input_x_domain_r, input_t_domain_r],axis=1), 'adap_resample.pt')\n",
    "                \n",
    "\n",
    "create_network(load=False, loadfile = \"lr0.001_t1_4_6_19h.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a748d9-0325-4be6-9344-20940f43a2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
