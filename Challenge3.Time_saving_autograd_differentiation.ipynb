{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "689fa29b",
   "metadata": {},
   "source": [
    "# Different input tensor shapes in `torch.autograd.grad` for economic running time\n",
    "\n",
    "***Using different algorithmic differentiation by `torch.autograd.grad` in PINN training resampling***\n",
    "**Woojeong Kim** *4/6/2025*\n",
    "\n",
    "- **Case Situation**: In neural network backward process, we should operate differentiation in various algorithmic way. Specifically, with given whole sampling points over domain, there are different mehtods for updating weight of the gradient descent by using mini-batch, gradient descent and stochastic descent. While these different updating methods spend much time for iterating each backward loss process, resampling with picking up the highest loss for backward process can be done with much economic way when we know the difference of the time requirements for several algorithmic design for loss computing and picking up the highest loss. The highly recommended method is making us of the vector/matrix form of tensor-i.e., higher dimension of the input tensor as much as possible- to minimize the iteratuin if loss computing. Also, for differentiation peratator in loss computation, we can efficiently use the function `torch.autograd.grad` with this high-dimensional tensor inputs.\n",
    "\\\\\n",
    "For example, given 2000 training points and assume that we will pick up the 200 highest loss with a loss formula and thus resample the training points brining out these 200 loss values. In this context, the steps are:\n",
    "  - **1. Input tensor shape on Loss function**: Before ranking the loss out of the 2000 points, though we should compute loss on 'each' of the point, we should minimize the number of passing the input through the loss function. For this, **we use the maximal number of the collocatin points for the updating loss which is processed after computing the resampled collocation points**. For example, if we are using the mini-bach gradient descent, it is recommended to use the number of input training points as batch size. For saving 8~10 times, this method is recommended.\n",
    "\n",
    "  - **2. Loss computation**: Usually in the loss function, we pass the input through differentiation with pytorch function `torch.autograd.grad`. Technically, instead of using `u_x = torch.autograd.grad(u.sum(), x, create_graph=True, retain_graph=True)[0]`, we use `u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]`. This is because the former one is losing vectorization form for the output u, and thus more time is required to run the code for processing many of training points input data. Meanwhile, the latter one maintains same vector dimension of the input training points tensor without wasting of running time.\n",
    "\n",
    "\n",
    "### Example:\n",
    "Here's a example code with all details of above 1. and 2..:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Input tensor shape on Loss function\n",
    "##Create a tensor with requires_grad=True to track operations for gradient computation\n",
    "x = np.random.uniform(low=-1, high=1, size=(2000,1))\n",
    "t = np.random.uniform(low=0, high=1, size=(2000,1))\n",
    "    \n",
    "input_x= Variable(torch.from_numpy(x).float(), requires_grad=True).to(device)\n",
    "input_t = Variable(torch.from_numpy(t).float(), requires_grad=True).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# 2. Loss computation\n",
    "loss_2000values = lossNSpde(net, input_x, input_t)\n",
    "\n",
    "def lossNSpde(net, x, t):\n",
    "    mse_cost_function = torch.nn.MSELoss()\n",
    "    \n",
    "    u = net(x, t)\n",
    "    zero = torch.zeros_like(x).to(device)\n",
    "    \n",
    "    #Compute Derivatives\n",
    "    \n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0] #Without using u.sum(), we can maintain the tensor dimension and vector form by 'grad_outputs=torch.ones_like(u)' form.\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "\n",
    "    #Loss functions w.r.t. governing Navier-Stokes equation on inner space\n",
    "    #AC\n",
    "    AC_Residual = u_t - 0.0001 * u_xx + 4*u**3 - 4*u\n",
    "    \n",
    "    #loss = mse_cost_function(AC_Residual, zero)\n",
    "    \n",
    "    return AC_Residual\n",
    "\n",
    "```\n",
    "Even for not that bic tarining collocation points with the number of several thousands, the different input form for loss function resulted in different running time from 8 times to 10times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a7a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
