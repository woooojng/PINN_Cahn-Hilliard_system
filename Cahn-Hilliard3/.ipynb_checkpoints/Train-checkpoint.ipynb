{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08eb5b1-3938-45c4-baf7-5e8c567196f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of Today :  3  / 2 \n",
      "Hour :  12\n",
      "Training PDE\n",
      "Executing Pass 1\n",
      "Current Final Time: 1 Current Learning Rate:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j9/4s8713nn53v3jz3ky1rz1f6h0000gn/T/ipykernel_85382/375446160.py:133: RuntimeWarning: divide by zero encountered in divide\n",
      "  a = psi_prop/psi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 1 \tTotal Loss: tensor(884.7590)\n",
      "\tphi IC Loss:  0.5468258857727051 \tphi IC Loss Adaptive:  0.33791741728782654 \tu BC Loss:  0.012057001702487469 \tu diff BC Loss:  0.00207639136351645 \n",
      "PDE Loss:  0.0014739467296749353 \tPDE Loss:  0.0014706498477607965\n",
      "  *Saved ; Early Stopping for the min CH PDE Loss\n",
      "\n",
      "\n",
      "Iteration: 500 \tTotal Loss: tensor(71.7248)\n",
      "\tphi IC Loss:  0.5467524528503418 \tphi IC Loss Adaptive:  0.16188164055347443 \tu BC Loss:  0.0007619967218488455 \tu diff BC Loss:  0.0015840049600228667 \n",
      "PDE Loss:  0.0006138317403383553 \tPDE Loss:  0.0006014319369569421\n",
      "  *Saved ; Early Stopping for the min CH PDE Loss\n",
      "\n",
      "  *Saved ; Early Stopping for the min Bd Loss\n",
      "\n",
      "\n",
      "Iteration: 1000 \tTotal Loss: tensor(8.5865)\n",
      "\tphi IC Loss:  0.5236272811889648 \tphi IC Loss Adaptive:  0.17781753838062286 \tu BC Loss:  3.14801000058651e-05 \tu diff BC Loss:  0.0009392068604938686 \n",
      "PDE Loss:  0.0002728007675614208 \tPDE Loss:  0.00027566670905798674\n",
      "  *Saved ; Early Stopping for the min CH PDE Loss\n",
      "\n",
      "  *Saved ; Early Stopping for the min Bd Loss\n",
      "\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "currentDateTime = datetime.now()\n",
    "print(\"Date of Today : \", currentDateTime.month, \" /\", currentDateTime.day, \"\\nHour : \", currentDateTime.hour) \n",
    "\n",
    "#Date of Today\n",
    "ctime = f\"{currentDateTime.month}_{currentDateTime.day}_{currentDateTime.hour}h\"\n",
    "#torch.autograd.detect_anomaly(check_nan=True)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Choose Test Problem\n",
    "from BM_WZ_CHnet import BM_WZ_CHnet\n",
    "Net = BM_WZ_CHnet\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#Create and Train the network\n",
    "def create_network(layers, preload = False, preload_name=''):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    net = Net(layers)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    if preload == True:\n",
    "        try:\n",
    "            net.load_state_dict(torch.load(preload_name, map_location=torch.device('cpu')))\n",
    "            print('Loaded Successfully')\n",
    "        except Exception as error:\n",
    "            print('Loading Failed: ', error)\n",
    "            pass\n",
    "        \n",
    "    time_vec = [0, 0, 0, 0]\n",
    "    \n",
    "    #Set final times for running training_loop\n",
    "    time_slices = np.array([1])\n",
    "    \n",
    "    global epsilon #used to track loss\n",
    "    epsilon = []\n",
    "    global a_u_set\n",
    "    a_u_set = []\n",
    "    global a_u_diff_set\n",
    "    a_u_diff_set = []\n",
    "    \n",
    "    global b_u_set\n",
    "    b_u_set = []\n",
    "    global c_ac_set\n",
    "    c_ac_set = []\n",
    "    \n",
    "    a_u = 1 ##10 \n",
    "    a_u_diff = 1\n",
    "    \n",
    "    b_u = 1000 ##100 \n",
    "    c_ac = 1 ###1/10 ##1000\n",
    "    \n",
    "    \n",
    "    print('Training PDE')\n",
    "    \n",
    "    iteration_vec = [90000]\n",
    "    learning_rate_vec = 1e-3 * np.ones_like(iteration_vec)\n",
    "    #r_vec = [30, 20, 10, 10]\n",
    "    \n",
    "    \n",
    "    for i in range(len(iteration_vec)):\n",
    "        #Set loop to optimize in progressively smaller learning rates\n",
    "        print(f'Executing Pass {i+1}')\n",
    "        iterations = iteration_vec[i]\n",
    "        learning_rate = learning_rate_vec[i]   \n",
    "        #r = r_vec[i] #determines sampling distribution decay rate\n",
    "        r=1 \n",
    "        \n",
    "        training_loop(net, time_slices, iterations, learning_rate, r, record_loss = 100, print_loss = 500, a_u = a_u, a_u_diff = a_u_diff, b_u = b_u, c_ac = c_ac )\n",
    "        torch.save(net.state_dict(), f\"CH_Benchmarks_Pass_{i+1}.pt\")\n",
    "        \n",
    "        time_slices = [time_slices[-1]]\n",
    "        \n",
    "        np.savetxt('epsilon.txt', epsilon)\n",
    "        time_vec[i] = time.time()\n",
    "\n",
    "\n",
    "    np.savetxt('epsilon.txt', epsilon)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Total Time:\\t\", end-start)\n",
    "    for i in range(len(iteration_vec)):\n",
    "        if i>0:\n",
    "            print(f'Pass {i+1} Time: {time_vec[i] - time_vec[i-1]}')\n",
    "        else:\n",
    "            print(f'Pass 1 Time: {time_vec[0] - start}')\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def exponential_time_sample(collocation, t_l, t_u, r):\n",
    "    t_pre = np.random.uniform(0,1, size=(collocation, 1))\n",
    "    \n",
    "    t = -np.log(1-t_pre+t_pre*np.exp(-r*(t_u-t_l)))/r\n",
    "    \n",
    "    return t\n",
    "\n",
    "def initialize_adaptive_x(net, x_l, x_u, size_IC, size_domain):\n",
    "    total_size = size_IC + size_domain\n",
    "    #Initialize\n",
    "    x = np.random.uniform(low=x_l, high=x_u, size=(total_size,1))\n",
    "    #y = np.random.uniform(low=y_l, high=y_u, size=(total_size,1))\n",
    "    \n",
    "    its = 10000\n",
    "    for i in range(its):\n",
    "        x_pt = Variable(torch.from_numpy(x).float(), requires_grad=True).to(device)\n",
    "        #y_pt = Variable(torch.from_numpy(y).float(), requires_grad=True).to(device)\n",
    "        \n",
    "        psi = net.W_exact(x_pt) #calculate density (scaled by constant)\n",
    "        psi = psi.detach().cpu().numpy()\n",
    "        \n",
    "        #First Proposal\n",
    "        x_prop = np.random.uniform(low=x_l, high=x_u, size=(total_size,1))\n",
    "        #y_prop = np.random.uniform(low=y_l, high=y_u, size=(total_size,1))\n",
    "        \n",
    "        x_prop_pt = Variable(torch.from_numpy(x_prop).float(), requires_grad=True).to(device)\n",
    "        #y_prop_pt = Variable(torch.from_numpy(y_prop).float(), requires_grad=True).to(device)\n",
    "        psi_prop = net.W_exact(x_prop_pt) #calculate density (scaled by constant)\n",
    "        psi_prop = psi_prop.detach().cpu().numpy()\n",
    "        \n",
    "        a = psi_prop/psi\n",
    "        \n",
    "        #accept worse proposals with probability a\n",
    "        rand = np.random.uniform(low=0, high=1, size=(total_size,1))\n",
    "        x[rand<a] = x_prop[rand<a]\n",
    "        #y[rand<a] = y_prop[rand<a]\n",
    "\n",
    "    x_IC, x_dom = np.split(x, [size_IC])\n",
    "    #y_IC, y_dom = np.split(y, [size_IC])\n",
    "    return x_IC, x_dom\n",
    "\n",
    "def step_adaptive_x(net, x_IC, x_dom, t_dom, x_l, x_u):\n",
    "    x_IC = x_IC.detach()\n",
    "    #y_IC = y_IC.detach()\n",
    "    x_dom = x_dom.detach() \n",
    "    #y_dom = y_dom.detach()\n",
    "    t_dom = t_dom.detach()\n",
    "    \n",
    "    Psi_IC = net.W_exact(x_IC)\n",
    "    Psi_dom = net.W(x_dom, t_dom)\n",
    "    \n",
    "    its = 1\n",
    "    for i in range(its):    \n",
    "        #First Proposal\n",
    "        x_prop_IC = (x_u-x_l)*torch.rand_like(x_IC) + x_l\n",
    "        #y_prop_IC = (y_u-y_l)*torch.rand_like(y_IC) + y_l\n",
    "        \n",
    "        x_prop_dom = (x_u-x_l)*torch.rand_like(x_dom) + x_l\n",
    "        #y_prop_dom = (y_u-y_l)*torch.rand_like(y_dom) + y_l\n",
    "        \n",
    "        psi_prop_IC = net.W_exact(x_prop_IC) #calculate density (scaled by constant)\n",
    "        psi_prop_dom = net.W(x_prop_dom, t_dom)\n",
    "        \n",
    "        a_IC = psi_prop_IC/Psi_IC\n",
    "        a_dom = psi_prop_dom/Psi_dom\n",
    "        \n",
    "        #accept worse proposals with probability a\n",
    "        rand_IC = torch.rand_like(x_IC)\n",
    "        rand_dom = torch.rand_like(x_dom)\n",
    "        \n",
    "        x_IC[rand_IC<a_IC] = x_prop_IC[rand_IC<a_IC]\n",
    "        #y_IC[rand_IC<a_IC] = y_prop_IC[rand_IC<a_IC]\n",
    "        \n",
    "        x_dom[rand_dom<a_dom] = x_prop_dom[rand_dom<a_dom]\n",
    "        #y_dom[rand_dom<a_dom] = y_prop_dom[rand_dom<a_dom]\n",
    "\n",
    "    return x_IC, x_dom\n",
    "\n",
    "def autograd_W(net, input):\n",
    "    list_grad = []\n",
    "    layers = [[net.layers_u[0].weight,net.layers_u[1].weight,net.layers_u[2].weight,net.layers_u[3].weight, net.layers_u[4].weight,net.layers_u[5].weight,net.layers_u[6].weight]]\n",
    "    #[net.hidden_layer1_u1.weight,net.hidden_layer2_u1.weight,net.hidden_layer3_u1.weight, net.hidden_layer4_u1.weight,net.output_layer_u1.weight],\n",
    "                #[net.hidden_layer1_u2.weight,net.hidden_layer2_u2.weight,net.hidden_layer3_u2.weight, net.hidden_layer4_u2.weight,net.output_layer_u2.weight], \n",
    "                 #[net.hidden_layer1_P.weight,net.hidden_layer2_P.weight,net.hidden_layer3_P.weight, net.hidden_layer4_P.weight,net.output_layer_P.weight],\n",
    "                  \n",
    "    for j in range(0, len(layers)):\n",
    "        for i in range(0,7):\n",
    "            diff = layers[j][i]\n",
    "            try:\n",
    "                result = torch.autograd.grad(input.sum(), diff, create_graph=True, retain_graph=True)[0]\n",
    "                list_grad.append(result)\n",
    "            except:\n",
    "                pass\n",
    "    return list_grad\n",
    "\n",
    "def training_loop(net, time_slices, iterations, learning_rate, r, record_loss, print_loss, a_u, a_u_diff, b_u, c_ac):\n",
    "    global epsilon #used to track and record loss\n",
    "    global a_u_set\n",
    "    global a_u_diff_set\n",
    "    global b_u_set\n",
    "    global c_ac_set\n",
    "    \n",
    "    # Domain boundary\n",
    "    x_l = net.x1_l\n",
    "    x_u = net.x1_u\n",
    "    \n",
    "    #y_l = net.x2_l\n",
    "    #y_u = net.x2_u\n",
    "\n",
    "    #time starts at lower bound 0, ends at upper bouund updated in slices\n",
    "    t_l = 0\n",
    "\n",
    "    # Generate following random numbers for x, y t\n",
    "    BC_collocation = int(100) #1000\n",
    "    IC_collocation = int(100) #5000\n",
    "    pde_collocation = int(10000) #10000\n",
    "    \n",
    "    IC_collocation_adaptive = int(400) #5000\n",
    "    pde_collocation_adaptive = int(40000) #10000\n",
    "    lamba = .9\n",
    "    #learning rate update\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    for final_time in time_slices:\n",
    "        min_loss = 20\n",
    "        min_mse_BC = .01\n",
    "        \n",
    "        min_mse_AC = .01\n",
    "        min_mse_IC = .01\n",
    "        \n",
    "        epoch = 0\n",
    "         \n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"Current Final Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        \n",
    "        #Initialize Adaptive Sampling points outside the loop\n",
    "        ##Define Colloacation Points with Initial Condition\n",
    "        \n",
    "        x_IC_adaptive, x_domain_adaptive = initialize_adaptive_x(net, x_l, x_u, IC_collocation_adaptive, pde_collocation_adaptive)\n",
    "\n",
    "        input_x_IC_adaptive = Variable(torch.from_numpy(x_IC_adaptive).float(), requires_grad=True).to(device)\n",
    "        #input_y_IC_adaptive = Variable(torch.from_numpy(y_IC_adaptive).float(), requires_grad=True).to(device)\n",
    "\n",
    "        #Define Inner Domain Collocation Points\n",
    "        t_domain_adaptive = exponential_time_sample(pde_collocation_adaptive, t_l, final_time, r)\n",
    "\n",
    "        input_x_domain_adaptive = Variable(torch.from_numpy(x_domain_adaptive).float(), requires_grad=True).to(device)\n",
    "        #input_y_domain_adaptive = Variable(torch.from_numpy(y_domain_adaptive).float(), requires_grad=True).to(device)\n",
    "        input_t_domain_adaptive = Variable(torch.from_numpy(t_domain_adaptive).float(), requires_grad=True).to(device)\n",
    "        \n",
    "        #grad_NS_loss_mean = 0.1 **5\n",
    "        #grad_NS_div_loss_mean = 0.1 **5\n",
    "        grad_AC_loss_mean = 1\n",
    "        \n",
    "        grad_BC_u_loss_mean = 1\n",
    "        grad_BC_u_diff_loss_mean = 1\n",
    "        grad_IC_u_loss_mean = 1\n",
    "        \n",
    "        \n",
    "        epoch = 0\n",
    "        while epoch <= iterations:\n",
    "            # Resetting gradients to zero            \n",
    "            net.optimizer.zero_grad()    \n",
    "            \n",
    "            if epoch > 0:\n",
    "                input_x_IC_adaptive, input_x_domain_adaptive = step_adaptive_x(net, input_x_IC_adaptive, input_x_domain_adaptive, input_t_domain_adaptive, x_l, x_u)\n",
    "                \n",
    "                input_x_IC_adaptive = Variable(input_x_IC_adaptive, requires_grad=True).to(device)\n",
    "                #input_y_IC_adaptive = Variable(input_y_IC_adaptive, requires_grad=True).to(device)\n",
    "                \n",
    "                input_x_domain_adaptive = Variable(input_x_domain_adaptive, requires_grad=True).to(device)\n",
    "                #input_y_domain_adaptive = Variable(input_y_domain_adaptive, requires_grad=True).to(device)\n",
    "                input_t_domain_adaptive = Variable(input_t_domain_adaptive, requires_grad=True).to(device)\n",
    "                \n",
    "            ##Define Colloacation Points with Initial Condition\n",
    "            x_IC = np.random.uniform(low=x_l, high=x_u, size=(IC_collocation,1))\n",
    "            #y_IC = np.random.uniform(low=y_l, high=y_u, size=(IC_collocation,1))\n",
    "\n",
    "            input_x_IC = Variable(torch.from_numpy(x_IC).float(), requires_grad=True).to(device)\n",
    "            #input_y_IC = Variable(torch.from_numpy(y_IC).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            ##Define Boundary Condition Collocation Points\n",
    "            x_BC = np.random.uniform(low=x_l, high=x_u, size=(BC_collocation,1))\n",
    "            #y_BC = np.random.uniform(low=y_l, high=y_u, size=(BC_collocation,1))       \n",
    "            t_BC = np.random.uniform(low=t_l, high=final_time, size=(BC_collocation,1))\n",
    "    \n",
    "            input_x_BC= Variable(torch.from_numpy(x_BC).float(), requires_grad=True).to(device)\n",
    "            #input_y_BC = Variable(torch.from_numpy(y_BC).float(), requires_grad=True).to(device)\n",
    "            input_t_BC = Variable(torch.from_numpy(t_BC).float(), requires_grad=True).to(device)\n",
    "    \n",
    "            #Define Inner Domain Collocation Points\n",
    "            x_domain = np.random.uniform(low= x_l, high=x_u, size=(pde_collocation, 1))\n",
    "            #y_domain = np.random.uniform(low= y_l, high=y_u, size=(pde_collocation, 1))\n",
    "            t_domain = exponential_time_sample(pde_collocation, t_l, final_time, r)\n",
    "            #t_domain = np.random.uniform(low= t_l, high=final_time, size=(pde_collocation, 1)) \n",
    "    \n",
    "            input_x_domain = Variable(torch.from_numpy(x_domain).float(), requires_grad=True).to(device)\n",
    "            #input_y_domain = Variable(torch.from_numpy(y_domain).float(), requires_grad=True).to(device)\n",
    "            input_t_domain = Variable(torch.from_numpy(t_domain).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            ### Training steps\n",
    "            ## Compute Loss\n",
    "            # Loss based on Initial Condition\n",
    "            mse_IC = net.Initial_Condition_Loss(input_x_IC)\n",
    "            mse_IC_adaptive = net.Initial_Condition_Loss(input_x_IC_adaptive)\n",
    "            \n",
    "            mse_IC_total = mse_IC + mse_IC_adaptive\n",
    "        \n",
    "            # Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "            mse_BC_u, mse_BC_u_diff = net.Boundary_Loss(input_x_BC, input_t_BC)\n",
    "        \n",
    "            # Loss based on PDE\n",
    "            mse_PDE = net.PDE_Loss(input_x_domain, input_t_domain)\n",
    "            mse_PDE_adaptive = net.PDE_Loss(input_x_domain_adaptive, input_t_domain_adaptive)\n",
    "            \n",
    "            mse_PDE_total = mse_PDE #+ mse_PDE_adaptive\n",
    "            \n",
    "            # Sum Loss\n",
    "            loss = b_u * mse_IC_total + a_u *mse_BC_u+ a_u_diff * mse_BC_u_diff + c_ac * mse_PDE_total\n",
    "            \n",
    "            \n",
    "            if epoch%500 == 0:\n",
    "                \n",
    "                #compute next step\n",
    "                \n",
    "                if c_ac < 5000 :\n",
    "                    mean_set = []\n",
    "                    for e in autograd_W(net, mse_PDE_total):\n",
    "                        m = torch.mean(torch.abs(e))\n",
    "                        mean_set.append(m.item())\n",
    "                    grad_AC_loss_mean = np.mean(mean_set)\n",
    "                \n",
    "                if a_u < 5000 :\n",
    "                    mean_set = []\n",
    "                    for e in autograd_W(net, mse_BC_u):\n",
    "                        m = torch.mean(torch.abs(e))\n",
    "                        mean_set.append(m.item())\n",
    "                    grad_BC_u_loss_mean = np.mean(mean_set)\n",
    "                \n",
    "                if a_u < 5000 :\n",
    "                    mean_set = []\n",
    "                    for e in autograd_W(net, mse_BC_u_diff):\n",
    "                        m = torch.mean(torch.abs(e))\n",
    "                        mean_set.append(m.item())\n",
    "                    grad_BC_u_diff_loss_mean = np.mean(mean_set)\n",
    "                \n",
    "                if b_u < 5000 :\n",
    "                    mean_set = []\n",
    "                    for e in autograd_W(net, mse_IC_total):\n",
    "                        m = torch.mean(torch.abs(e))\n",
    "                        mean_set.append(m.item())\n",
    "                    grad_IC_u_loss_mean = np.mean(mean_set)\n",
    "                \n",
    "                grad_mean_sum = (grad_BC_u_loss_mean + grad_BC_u_diff_loss_mean + grad_IC_u_loss_mean+ grad_AC_loss_mean )\n",
    "                \n",
    "                \n",
    "                # coeffiecient of BC => a, coeffiecient of IC => b\n",
    "                a_u_next = (grad_mean_sum)/(grad_BC_u_loss_mean)\n",
    "                a_u_diff_next = (grad_mean_sum)/(grad_BC_u_diff_loss_mean)\n",
    "                \n",
    "                b_u_next = (grad_mean_sum)/(grad_IC_u_loss_mean)\n",
    "                \n",
    "                c_ac_next = (grad_mean_sum)/(grad_AC_loss_mean)\n",
    "                \n",
    "                # #update rates\n",
    "                if a_u_next < 5000:\n",
    "                    a_u = (1-lamba) * a_u + lamba * a_u_next\n",
    "                if a_u_diff_next < 5000:\n",
    "                    a_u_diff = (1-lamba) * a_u_diff + lamba * a_u_diff_next\n",
    "                \n",
    "                if b_u_next < 5000:\n",
    "                    b_u = (1-lamba) * b_u + lamba * b_u_next\n",
    "                if c_ac_next < 5000:\n",
    "                    c_ac = (1-lamba) * c_ac + lamba * c_ac_next\n",
    "                \n",
    "            \n",
    "            ## Compute Gradients\n",
    "            # Compute Actual Gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            #Gradient Norm Clipping\n",
    "            #torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm = 1000, error_if_nonfinite=False) #note max norm is more art than science\n",
    "            \n",
    "            def closure():\n",
    "                return loss\n",
    "            \n",
    "            ## Step Optimizer\n",
    "            #net.optimizer.step(closure)\n",
    "            net.optimizer.step()\n",
    "            \n",
    "            ### Update Epoch\n",
    "            epoch = epoch + 1\n",
    "            \n",
    "            ### Save and Print\n",
    "            \n",
    "            # if epoch < 10:\n",
    "            #     #save adaptive points\n",
    "            #     torch.save(input_x_IC_adaptive, f'x_IC_adaptive_{epoch}.pt')\n",
    "            #     torch.save(input_y_IC_adaptive, f'y_IC_adaptive_{epoch}.pt')\n",
    "            #     torch.save(input_x_domain_adaptive, f'x_dom_adaptive_{epoch}.pt')\n",
    "            #     torch.save(input_y_domain_adaptive, f'y_dom_adaptive_{epoch}.pt')\n",
    "            #     torch.save(input_t_domain_adaptive, f't_dom_adaptive_{epoch}.pt')\n",
    "            #Print Loss every 1000 Epochs\n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch == 1 or epoch%record_loss == 0:\n",
    "                    epsilon = np.append(epsilon, loss.cpu().detach().numpy())\n",
    "                    a_u_set = np.append(a_u_set, a_u)\n",
    "                    a_u_diff_set = np.append(a_u_diff_set, a_u_diff)\n",
    "                    b_u_set = np.append(b_u_set, b_u)\n",
    "                    c_ac_set = np.append(c_ac_set, c_ac)\n",
    "                if epoch == 1 or epoch%print_loss == 0:\n",
    "                    print(\"\\nIteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    print(\"\\tphi IC Loss: \", mse_IC.item(),\"\\tphi IC Loss Adaptive: \", mse_IC_adaptive.item(),\"\\tu BC Loss: \", mse_BC_u.item(), \n",
    "                          \"\\tu diff BC Loss: \", mse_BC_u_diff.item(), \"\\nPDE Loss: \", mse_PDE.item(), \"\\tPDE Loss: \", mse_PDE_adaptive.item())\n",
    "                    \n",
    "                    if mse_PDE_total.cpu().detach().numpy() < min_mse_AC:\n",
    "                        min_mse_AC = mse_PDE_total.cpu().detach().numpy()\n",
    "                        torch.save(net.state_dict(), f\"ES_min_CHloss_lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                        print('  *Saved ; Early Stopping for the min CH PDE Loss\\n')\n",
    "                    if mse_BC_u.cpu().detach().numpy() < min_mse_BC:\n",
    "                        min_mse_BC = mse_BC_u.cpu().detach().numpy()\n",
    "                        torch.save(net.state_dict(), f\"ES_min_mse_Bd_lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                        print('  *Saved ; Early Stopping for the min Bd Loss\\n')\n",
    "                    if mse_IC_total.cpu().detach().numpy() < min_mse_IC:\n",
    "                        min_mse_IC = mse_IC_total.cpu().detach().numpy()\n",
    "                        torch.save(net.state_dict(), f\"ES_min_mse_IC_lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                        print('  *Saved ; Early Stopping for the min IC Loss\\n')\n",
    "                    if loss.cpu().detach().numpy() < min_loss:\n",
    "                        min_loss = loss.cpu().detach().numpy()\n",
    "                        torch.save(net.state_dict(), f\"ES_min_loss_lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                        print('  *Saved ; Early Stopping for the Minimal Total Loss\\n')\n",
    "                if epoch%6000 == 0:\n",
    "                    np.savetxt(f\"{ctime}epsilon.txt\", epsilon)\n",
    "                    np.savetxt(f\"{ctime}_a_u.txt\", a_u_set)\n",
    "                    np.savetxt(f\"{ctime}_a_u_diff.txt\", a_u_diff_set)\n",
    "                    np.savetxt(f\"{ctime}_b_u.txt\", b_u_set)\n",
    "                    np.savetxt(f\"{ctime}_c_ac.txt\", c_ac_set)\n",
    "                    torch.save(net.state_dict(), f\"lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                    \n",
    "    \n",
    "layers=[2, 64, 64, 64, 64, 64, 64, 1]\n",
    "create_network(layers, preload=False, preload_name='WZ_Regularized_BFGS.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76516c98-2e4a-4c24-85a9-06934d6b6b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
