{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08eb5b1-3938-45c4-baf7-5e8c567196f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of Today :  3  / 6 \n",
      "Hour :  10\n",
      "Loaded Successfully\n",
      "Training PDE\n",
      "Executing Pass 1\n",
      "Current Final Time: 1 Current Learning Rate:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j9/4s8713nn53v3jz3ky1rz1f6h0000gn/T/ipykernel_2098/830190865.py:133: RuntimeWarning: divide by zero encountered in divide\n",
      "  a = psi_prop/psi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 1 \tTotal Loss: tensor(2.1281)\n",
      "\tphi IC Loss:  1.759724455041578e-06 \tphi IC Loss Adaptive:  3.24731740874995e-06 \tu BC Loss:  0.002944197040051222 \tu diff BC Loss:  0.008284034207463264 \n",
      "PDE Loss:  2.111903429031372 \tPDE Loss:  5.063554286956787\n",
      "  *Saved ; Early Stopping for the min Bd Loss\n",
      "\n",
      "  *Saved ; Early Stopping for the min IC Loss\n",
      "\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 500 \tTotal Loss: tensor(0.3020)\n",
      "\tphi IC Loss:  2.0627032427000813e-05 \tphi IC Loss Adaptive:  2.3899545340100303e-05 \tu BC Loss:  0.0002260564360767603 \tu diff BC Loss:  0.0008047778974287212 \n",
      "PDE Loss:  0.25643688440322876 \tPDE Loss:  0.3630935847759247\n",
      "  *Saved ; Early Stopping for the min Bd Loss\n",
      "\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 1000 \tTotal Loss: tensor(0.3348)\n",
      "\tphi IC Loss:  1.6710406271158718e-05 \tphi IC Loss Adaptive:  5.676093860529363e-05 \tu BC Loss:  0.00026840242207981646 \tu diff BC Loss:  0.0013995026238262653 \n",
      "PDE Loss:  0.25967371463775635 \tPDE Loss:  0.3404904007911682\n",
      "\n",
      "Iteration: 1500 \tTotal Loss: tensor(1.1855)\n",
      "\tphi IC Loss:  0.00032062490936368704 \tphi IC Loss Adaptive:  0.0006202643271535635 \tu BC Loss:  0.000791570171713829 \tu diff BC Loss:  0.0008026831783354282 \n",
      "PDE Loss:  0.24296872317790985 \tPDE Loss:  0.35290223360061646\n",
      "\n",
      "Iteration: 2000 \tTotal Loss: tensor(1.5870)\n",
      "\tphi IC Loss:  0.0003640684299170971 \tphi IC Loss Adaptive:  0.0009852929506450891 \tu BC Loss:  0.0009824398439377546 \tu diff BC Loss:  0.0006378460093401372 \n",
      "PDE Loss:  0.23605245351791382 \tPDE Loss:  0.3667010962963104\n",
      "\n",
      "Iteration: 2500 \tTotal Loss: tensor(5.0532)\n",
      "\tphi IC Loss:  0.0006095291464589536 \tphi IC Loss Adaptive:  0.004189088940620422 \tu BC Loss:  0.0004145804268773645 \tu diff BC Loss:  0.0027701002545654774 \n",
      "PDE Loss:  0.2513713240623474 \tPDE Loss:  0.361595094203949\n",
      "\n",
      "Iteration: 3000 \tTotal Loss: tensor(0.2301)\n",
      "\tphi IC Loss:  3.50261211679026e-06 \tphi IC Loss Adaptive:  1.0181121069763321e-05 \tu BC Loss:  0.00011124424781883135 \tu diff BC Loss:  9.974922431865707e-05 \n",
      "PDE Loss:  0.21623875200748444 \tPDE Loss:  0.32617858052253723\n",
      "  *Saved ; Early Stopping for the min Bd Loss\n",
      "\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 3500 \tTotal Loss: tensor(0.3225)\n",
      "\tphi IC Loss:  3.7223388062557206e-05 \tphi IC Loss Adaptive:  4.9252816097578034e-05 \tu BC Loss:  0.0003031794331036508 \tu diff BC Loss:  0.0018202444771304727 \n",
      "PDE Loss:  0.2339305877685547 \tPDE Loss:  0.3370230793952942\n",
      "\n",
      "Iteration: 4000 \tTotal Loss: tensor(8.6748)\n",
      "\tphi IC Loss:  0.0024778260849416256 \tphi IC Loss Adaptive:  0.0059106070548295975 \tu BC Loss:  0.0020798142068088055 \tu diff BC Loss:  0.014465944841504097 \n",
      "PDE Loss:  0.2697807550430298 \tPDE Loss:  0.36004820466041565\n",
      "\n",
      "Iteration: 4500 \tTotal Loss: tensor(0.3023)\n",
      "\tphi IC Loss:  2.2898500901646912e-05 \tphi IC Loss Adaptive:  5.895742651773617e-05 \tu BC Loss:  0.0001612502383068204 \tu diff BC Loss:  0.0003170363197568804 \n",
      "PDE Loss:  0.21999533474445343 \tPDE Loss:  0.3220992982387543\n",
      "\n",
      "Iteration: 5000 \tTotal Loss: tensor(0.2435)\n",
      "\tphi IC Loss:  7.807656402292196e-06 \tphi IC Loss Adaptive:  8.886452633305453e-06 \tu BC Loss:  9.245509136235341e-05 \tu diff BC Loss:  0.00011893730697920546 \n",
      "PDE Loss:  0.22655066847801208 \tPDE Loss:  0.3237640857696533\n",
      "  *Saved ; Early Stopping for the min Bd Loss\n",
      "\n",
      "\n",
      "Iteration: 5500 \tTotal Loss: tensor(0.2824)\n",
      "\tphi IC Loss:  2.31840258493321e-05 \tphi IC Loss Adaptive:  3.171978460159153e-05 \tu BC Loss:  0.0002727129322011024 \tu diff BC Loss:  0.002256918465718627 \n",
      "PDE Loss:  0.22491934895515442 \tPDE Loss:  0.3221013844013214\n",
      "\n",
      "Iteration: 6000 \tTotal Loss: tensor(0.2657)\n",
      "\tphi IC Loss:  6.67725907987915e-06 \tphi IC Loss Adaptive:  6.954205673537217e-06 \tu BC Loss:  7.977268978720531e-05 \tu diff BC Loss:  0.0010936983162537217 \n",
      "PDE Loss:  0.2508566081523895 \tPDE Loss:  0.3324243128299713\n",
      "  *Saved ; Early Stopping for the min Bd Loss\n",
      "\n",
      "\n",
      "Iteration: 6500 \tTotal Loss: tensor(0.3681)\n",
      "\tphi IC Loss:  3.214786556782201e-05 \tphi IC Loss Adaptive:  0.00010437013406772166 \tu BC Loss:  0.00010267262405250221 \tu diff BC Loss:  0.0002557395491749048 \n",
      "PDE Loss:  0.23124520480632782 \tPDE Loss:  0.3351956903934479\n",
      "\n",
      "Iteration: 7000 \tTotal Loss: tensor(0.2441)\n",
      "\tphi IC Loss:  5.492157470143866e-06 \tphi IC Loss Adaptive:  6.83526513967081e-06 \tu BC Loss:  0.00027705836691893637 \tu diff BC Loss:  0.0012995911529287696 \n",
      "PDE Loss:  0.23023159801959991 \tPDE Loss:  0.32198554277420044\n",
      "\n",
      "Iteration: 7500 \tTotal Loss: tensor(0.3493)\n",
      "\tphi IC Loss:  2.871181641239673e-05 \tphi IC Loss Adaptive:  0.00011597213597269729 \tu BC Loss:  8.915660146158189e-05 \tu diff BC Loss:  0.00014472748443949968 \n",
      "PDE Loss:  0.20441363751888275 \tPDE Loss:  0.3258399963378906\n",
      "\n",
      "Iteration: 8000 \tTotal Loss: tensor(0.2321)\n",
      "\tphi IC Loss:  4.946358330926159e-06 \tphi IC Loss Adaptive:  8.98506368685048e-06 \tu BC Loss:  0.00015366454317700118 \tu diff BC Loss:  0.0003391113714314997 \n",
      "PDE Loss:  0.21772372722625732 \tPDE Loss:  0.3201664984226227\n",
      "\n",
      "Iteration: 8500 \tTotal Loss: tensor(0.3489)\n",
      "\tphi IC Loss:  3.29862414218951e-05 \tphi IC Loss Adaptive:  9.254242468159646e-05 \tu BC Loss:  0.00014278800517786294 \tu diff BC Loss:  0.0009771112818270922 \n",
      "PDE Loss:  0.22226250171661377 \tPDE Loss:  0.320843905210495\n",
      "\n",
      "Iteration: 9000 \tTotal Loss: tensor(0.4999)\n",
      "\tphi IC Loss:  6.78754149703309e-05 \tphi IC Loss Adaptive:  0.00022125821851659566 \tu BC Loss:  1.594720197317656e-05 \tu diff BC Loss:  0.0024111613165587187 \n",
      "PDE Loss:  0.20838391780853271 \tPDE Loss:  0.32167866826057434\n",
      "  *Saved ; Early Stopping for the min Bd Loss\n",
      "\n",
      "\n",
      "Iteration: 9500 \tTotal Loss: tensor(0.2415)\n",
      "\tphi IC Loss:  5.145839622855419e-06 \tphi IC Loss Adaptive:  5.586522092926316e-06 \tu BC Loss:  0.00018531134992372245 \tu diff BC Loss:  9.893099922919646e-05 \n",
      "PDE Loss:  0.23053312301635742 \tPDE Loss:  0.32033053040504456\n",
      "\n",
      "Iteration: 10000 \tTotal Loss: tensor(0.2423)\n",
      "\tphi IC Loss:  4.234844254824566e-06 \tphi IC Loss Adaptive:  5.198901817493606e-06 \tu BC Loss:  0.0001404666545568034 \tu diff BC Loss:  7.312611705856398e-05 \n",
      "PDE Loss:  0.232621431350708 \tPDE Loss:  0.3198750615119934\n",
      "\n",
      "Iteration: 10500 \tTotal Loss: tensor(0.3291)\n",
      "\tphi IC Loss:  2.7901434805244207e-05 \tphi IC Loss Adaptive:  7.962220115587115e-05 \tu BC Loss:  9.874079842120409e-05 \tu diff BC Loss:  0.0008672232506796718 \n",
      "PDE Loss:  0.22062994539737701 \tPDE Loss:  0.3110080063343048\n",
      "\n",
      "Iteration: 11000 \tTotal Loss: tensor(0.7436)\n",
      "\tphi IC Loss:  0.000155427391291596 \tphi IC Loss Adaptive:  0.0003573684371076524 \tu BC Loss:  0.0005499104736372828 \tu diff BC Loss:  0.006829662714153528 \n",
      "PDE Loss:  0.2233901470899582 \tPDE Loss:  0.3178795278072357\n",
      "\n",
      "Iteration: 11500 \tTotal Loss: tensor(0.6527)\n",
      "\tphi IC Loss:  0.0001441199128748849 \tphi IC Loss Adaptive:  0.00028119530179537833 \tu BC Loss:  8.923797577153891e-05 \tu diff BC Loss:  0.008063426241278648 \n",
      "PDE Loss:  0.2192237377166748 \tPDE Loss:  0.3133050501346588\n",
      "\n",
      "Iteration: 12000 \tTotal Loss: tensor(0.2633)\n",
      "\tphi IC Loss:  1.3966148799227085e-05 \tphi IC Loss Adaptive:  2.289816620759666e-05 \tu BC Loss:  9.831333591137081e-05 \tu diff BC Loss:  0.0001864093792391941 \n",
      "PDE Loss:  0.22619035840034485 \tPDE Loss:  0.33139458298683167\n",
      "\n",
      "Iteration: 12500 \tTotal Loss: tensor(4.6350)\n",
      "\tphi IC Loss:  0.0020341298077255487 \tphi IC Loss Adaptive:  0.002255677478387952 \tu BC Loss:  0.013665705919265747 \tu diff BC Loss:  0.06556922942399979 \n",
      "PDE Loss:  0.2659931778907776 \tPDE Loss:  0.36534038186073303\n",
      "\n",
      "Iteration: 13000 \tTotal Loss: tensor(0.5386)\n",
      "\tphi IC Loss:  0.0001405847870046273 \tphi IC Loss Adaptive:  0.00018144438217859715 \tu BC Loss:  0.0002453156921546906 \tu diff BC Loss:  0.0004306835471652448 \n",
      "PDE Loss:  0.21589376032352448 \tPDE Loss:  0.31917133927345276\n",
      "\n",
      "Iteration: 13500 \tTotal Loss: tensor(0.2440)\n",
      "\tphi IC Loss:  5.621441232506186e-06 \tphi IC Loss Adaptive:  1.08062395156594e-05 \tu BC Loss:  9.750766184879467e-05 \tu diff BC Loss:  0.0002891007752623409 \n",
      "PDE Loss:  0.22719120979309082 \tPDE Loss:  0.3121052086353302\n",
      "\n",
      "Iteration: 14000 \tTotal Loss: tensor(6.9022)\n",
      "\tphi IC Loss:  0.0028042562771588564 \tphi IC Loss Adaptive:  0.0038622491993010044 \tu BC Loss:  0.0015529087977483869 \tu diff BC Loss:  0.01063013356178999 \n",
      "PDE Loss:  0.223550945520401 \tPDE Loss:  0.3133522868156433\n",
      "\n",
      "Iteration: 14500 \tTotal Loss: tensor(1.1897)\n",
      "\tphi IC Loss:  0.00029216770781204104 \tphi IC Loss Adaptive:  0.0006563620991073549 \tu BC Loss:  2.080040030705277e-05 \tu diff BC Loss:  0.0011913834605365992 \n",
      "PDE Loss:  0.23999644815921783 \tPDE Loss:  0.3259824216365814\n",
      "\n",
      "Iteration: 15000 \tTotal Loss: tensor(0.2188)\n",
      "\tphi IC Loss:  1.3059116099611856e-06 \tphi IC Loss Adaptive:  3.7749350667581894e-06 \tu BC Loss:  9.103493357542902e-05 \tu diff BC Loss:  8.905671711545438e-05 \n",
      "PDE Loss:  0.21355800330638885 \tPDE Loss:  0.31706181168556213\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 15500 \tTotal Loss: tensor(6.0475)\n",
      "\tphi IC Loss:  0.0013920783530920744 \tphi IC Loss Adaptive:  0.004210500046610832 \tu BC Loss:  0.0003650060389190912 \tu diff BC Loss:  0.014874058775603771 \n",
      "PDE Loss:  0.429655522108078 \tPDE Loss:  0.6644940972328186\n",
      "\n",
      "Iteration: 16000 \tTotal Loss: tensor(0.2445)\n",
      "\tphi IC Loss:  4.628767783287913e-06 \tphi IC Loss Adaptive:  1.6469719412270933e-05 \tu BC Loss:  0.0001329194928985089 \tu diff BC Loss:  0.00010446035594213754 \n",
      "PDE Loss:  0.22320520877838135 \tPDE Loss:  0.3188937306404114\n",
      "\n",
      "Iteration: 16500 \tTotal Loss: tensor(0.2342)\n",
      "\tphi IC Loss:  5.520858849195065e-06 \tphi IC Loss Adaptive:  9.589059118297882e-06 \tu BC Loss:  6.122452032286674e-05 \tu diff BC Loss:  0.0003353531064931303 \n",
      "PDE Loss:  0.2186920940876007 \tPDE Loss:  0.31624892354011536\n",
      "\n",
      "Iteration: 17000 \tTotal Loss: tensor(0.3381)\n",
      "\tphi IC Loss:  5.260823672870174e-05 \tphi IC Loss Adaptive:  4.3282412661938e-05 \tu BC Loss:  0.0009068151121027768 \tu diff BC Loss:  0.00020618359849322587 \n",
      "PDE Loss:  0.24108478426933289 \tPDE Loss:  0.3261338472366333\n",
      "\n",
      "Iteration: 17500 \tTotal Loss: tensor(0.2142)\n",
      "\tphi IC Loss:  1.0670814845070709e-06 \tphi IC Loss Adaptive:  2.370067250012653e-06 \tu BC Loss:  7.911143620731309e-05 \tu diff BC Loss:  0.0002077894314425066 \n",
      "PDE Loss:  0.21046358346939087 \tPDE Loss:  0.3154248893260956\n",
      "  *Saved ; Early Stopping for the min IC Loss\n",
      "\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 18000 \tTotal Loss: tensor(0.2190)\n",
      "\tphi IC Loss:  2.8386127723933896e-06 \tphi IC Loss Adaptive:  9.125382348429412e-06 \tu BC Loss:  6.724785635014996e-05 \tu diff BC Loss:  2.677908014447894e-05 \n",
      "PDE Loss:  0.20692764222621918 \tPDE Loss:  0.3160090148448944\n",
      "\n",
      "Iteration: 18500 \tTotal Loss: tensor(0.2340)\n",
      "\tphi IC Loss:  9.374161891173571e-06 \tphi IC Loss Adaptive:  1.073038947652094e-05 \tu BC Loss:  0.00012060804147040471 \tu diff BC Loss:  0.00019825990602839738 \n",
      "PDE Loss:  0.21355374157428741 \tPDE Loss:  0.31401193141937256\n",
      "\n",
      "Iteration: 19000 \tTotal Loss: tensor(0.2366)\n",
      "\tphi IC Loss:  4.6361901695490815e-06 \tphi IC Loss Adaptive:  8.494667781633325e-06 \tu BC Loss:  4.268842894816771e-05 \tu diff BC Loss:  0.00023696031712461263 \n",
      "PDE Loss:  0.22314687073230743 \tPDE Loss:  0.3148795962333679\n",
      "\n",
      "Iteration: 19500 \tTotal Loss: tensor(0.2924)\n",
      "\tphi IC Loss:  1.552832873130683e-05 \tphi IC Loss Adaptive:  5.801956285722554e-05 \tu BC Loss:  0.00011934751091757789 \tu diff BC Loss:  7.963014650158584e-05 \n",
      "PDE Loss:  0.21866627037525177 \tPDE Loss:  0.31306788325309753\n",
      "\n",
      "Iteration: 20000 \tTotal Loss: tensor(4.4380)\n",
      "\tphi IC Loss:  0.0007750358781777322 \tphi IC Loss Adaptive:  0.0032555251382291317 \tu BC Loss:  0.0009370983461849391 \tu diff BC Loss:  0.042358916252851486 \n",
      "PDE Loss:  0.3641825318336487 \tPDE Loss:  0.44916611909866333\n",
      "\n",
      "Iteration: 20500 \tTotal Loss: tensor(0.2222)\n",
      "\tphi IC Loss:  4.491196705203038e-06 \tphi IC Loss Adaptive:  1.6981350654532434e-06 \tu BC Loss:  1.904622149595525e-05 \tu diff BC Loss:  0.0008754983427934349 \n",
      "PDE Loss:  0.21507500112056732 \tPDE Loss:  0.31722986698150635\n",
      "\n",
      "Iteration: 21000 \tTotal Loss: tensor(5.7429)\n",
      "\tphi IC Loss:  0.000990689848549664 \tphi IC Loss Adaptive:  0.004311463795602322 \tu BC Loss:  0.00011730907863238826 \tu diff BC Loss:  0.0015501284506171942 \n",
      "PDE Loss:  0.4390982985496521 \tPDE Loss:  0.4233386218547821\n",
      "\n",
      "Iteration: 21500 \tTotal Loss: tensor(0.2441)\n",
      "\tphi IC Loss:  8.025394890864845e-06 \tphi IC Loss Adaptive:  2.8719328838633373e-05 \tu BC Loss:  5.322401921148412e-05 \tu diff BC Loss:  9.369383042212576e-05 \n",
      "PDE Loss:  0.20718777179718018 \tPDE Loss:  0.3093114495277405\n",
      "\n",
      "Iteration: 22000 \tTotal Loss: tensor(0.3399)\n",
      "\tphi IC Loss:  2.697087802516762e-05 \tphi IC Loss Adaptive:  0.00011043643462471664 \tu BC Loss:  7.854148134356365e-05 \tu diff BC Loss:  0.0007786541245877743 \n",
      "PDE Loss:  0.20162279903888702 \tPDE Loss:  0.3134019076824188\n",
      "\n",
      "Iteration: 22500 \tTotal Loss: tensor(0.4832)\n",
      "\tphi IC Loss:  3.756964360945858e-05 \tphi IC Loss Adaptive:  0.00022028402599971741 \tu BC Loss:  6.231133738765493e-05 \tu diff BC Loss:  0.0002187388454331085 \n",
      "PDE Loss:  0.22501583397388458 \tPDE Loss:  0.3095809519290924\n",
      "\n",
      "Iteration: 23000 \tTotal Loss: tensor(0.2617)\n",
      "\tphi IC Loss:  9.81822631729301e-06 \tphi IC Loss Adaptive:  2.8415097403922118e-05 \tu BC Loss:  2.0078707166248932e-05 \tu diff BC Loss:  9.976710134651512e-05 \n",
      "PDE Loss:  0.22335387766361237 \tPDE Loss:  0.3111940920352936\n",
      "\n",
      "Iteration: 23500 \tTotal Loss: tensor(0.2247)\n",
      "\tphi IC Loss:  4.669131158152595e-06 \tphi IC Loss Adaptive:  9.050920198205858e-06 \tu BC Loss:  5.1424529374344274e-05 \tu diff BC Loss:  0.0006337848608382046 \n",
      "PDE Loss:  0.21032175421714783 \tPDE Loss:  0.32323768734931946\n",
      "\n",
      "Iteration: 24000 \tTotal Loss: tensor(4.7926)\n",
      "\tphi IC Loss:  0.0008463645935989916 \tphi IC Loss Adaptive:  0.0036921806167811155 \tu BC Loss:  2.095078343700152e-05 \tu diff BC Loss:  0.004887761548161507 \n",
      "PDE Loss:  0.24910108745098114 \tPDE Loss:  0.36725857853889465\n",
      "\n",
      "Iteration: 24500 \tTotal Loss: tensor(0.2141)\n",
      "\tphi IC Loss:  1.3784429029328749e-06 \tphi IC Loss Adaptive:  5.853412858414231e-06 \tu BC Loss:  2.902562300732825e-05 \tu diff BC Loss:  0.0001568896695971489 \n",
      "PDE Loss:  0.20670919120311737 \tPDE Loss:  0.311473548412323\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 25000 \tTotal Loss: tensor(0.4754)\n",
      "\tphi IC Loss:  9.409007907379419e-05 \tphi IC Loss Adaptive:  0.00014565449964720756 \tu BC Loss:  6.237880734261125e-05 \tu diff BC Loss:  0.0008013930055312812 \n",
      "PDE Loss:  0.23479469120502472 \tPDE Loss:  0.32423678040504456\n",
      "\n",
      "Iteration: 25500 \tTotal Loss: tensor(0.2525)\n",
      "\tphi IC Loss:  9.489776857662946e-06 \tphi IC Loss Adaptive:  3.5697827115654945e-05 \tu BC Loss:  2.915083314292133e-05 \tu diff BC Loss:  0.0003336512017995119 \n",
      "PDE Loss:  0.20690850913524628 \tPDE Loss:  0.31347379088401794\n",
      "\n",
      "Iteration: 26000 \tTotal Loss: tensor(7.0773)\n",
      "\tphi IC Loss:  0.0017494683852419257 \tphi IC Loss Adaptive:  0.0038009597919881344 \tu BC Loss:  0.00031619990477338433 \tu diff BC Loss:  0.038468848913908005 \n",
      "PDE Loss:  1.4881157875061035 \tPDE Loss:  0.5279238224029541\n",
      "\n",
      "Iteration: 26500 \tTotal Loss: tensor(0.2266)\n",
      "\tphi IC Loss:  2.544744575061486e-06 \tphi IC Loss Adaptive:  4.3430250116216484e-06 \tu BC Loss:  1.7605141692911275e-05 \tu diff BC Loss:  0.00013513676822185516 \n",
      "PDE Loss:  0.21954980492591858 \tPDE Loss:  0.3266608417034149\n",
      "\n",
      "Iteration: 27000 \tTotal Loss: tensor(0.5227)\n",
      "\tphi IC Loss:  0.00011023849219782278 \tphi IC Loss Adaptive:  0.00019286306633148342 \tu BC Loss:  6.443095480790362e-05 \tu diff BC Loss:  0.0004888217663392425 \n",
      "PDE Loss:  0.2190328985452652 \tPDE Loss:  0.3124951124191284\n",
      "\n",
      "Iteration: 27500 \tTotal Loss: tensor(0.2398)\n",
      "\tphi IC Loss:  6.92045250616502e-06 \tphi IC Loss Adaptive:  6.387761914083967e-06 \tu BC Loss:  7.45309516787529e-05 \tu diff BC Loss:  0.0006331871845759451 \n",
      "PDE Loss:  0.22576288878917694 \tPDE Loss:  0.31832316517829895\n",
      "\n",
      "Iteration: 28000 \tTotal Loss: tensor(0.2411)\n",
      "\tphi IC Loss:  1.0207958439423237e-05 \tphi IC Loss Adaptive:  1.9165585399605334e-05 \tu BC Loss:  7.986491254996508e-05 \tu diff BC Loss:  9.85468941507861e-05 \n",
      "PDE Loss:  0.21158109605312347 \tPDE Loss:  0.3034408688545227\n",
      "\n",
      "Iteration: 28500 \tTotal Loss: tensor(0.8328)\n",
      "\tphi IC Loss:  0.00012859755952376872 \tphi IC Loss Adaptive:  0.0004827778320759535 \tu BC Loss:  9.81310149654746e-05 \tu diff BC Loss:  0.000699096592143178 \n",
      "PDE Loss:  0.22061321139335632 \tPDE Loss:  0.30866217613220215\n",
      "\n",
      "Iteration: 29000 \tTotal Loss: tensor(1.4674)\n",
      "\tphi IC Loss:  0.00035705132177099586 \tphi IC Loss Adaptive:  0.0008497670642100275 \tu BC Loss:  0.0008190971566364169 \tu diff BC Loss:  0.0008492001215927303 \n",
      "PDE Loss:  0.25893324613571167 \tPDE Loss:  0.3454144299030304\n",
      "\n",
      "Iteration: 29500 \tTotal Loss: tensor(0.2208)\n",
      "\tphi IC Loss:  1.6856872662174283e-06 \tphi IC Loss Adaptive:  1.999434971367009e-06 \tu BC Loss:  2.0981216948712245e-05 \tu diff BC Loss:  0.00015851858188398182 \n",
      "PDE Loss:  0.21690542995929718 \tPDE Loss:  0.3129337728023529\n",
      "\n",
      "Iteration: 30000 \tTotal Loss: tensor(5.2496)\n",
      "\tphi IC Loss:  0.0022676417138427496 \tphi IC Loss Adaptive:  0.0024608680978417397 \tu BC Loss:  0.002615851117298007 \tu diff BC Loss:  0.06160559132695198 \n",
      "PDE Loss:  0.45691657066345215 \tPDE Loss:  0.6799906492233276\n",
      "\n",
      "Iteration: 30500 \tTotal Loss: tensor(0.2440)\n",
      "\tphi IC Loss:  2.5479437226749724e-06 \tphi IC Loss Adaptive:  2.2889482352184132e-05 \tu BC Loss:  4.689073102781549e-05 \tu diff BC Loss:  0.00011438041838118806 \n",
      "PDE Loss:  0.2183789312839508 \tPDE Loss:  0.3206702172756195\n",
      "\n",
      "Iteration: 31000 \tTotal Loss: tensor(42.2016)\n",
      "\tphi IC Loss:  0.012869161553680897 \tphi IC Loss Adaptive:  0.023939840495586395 \tu BC Loss:  0.3250279724597931 \tu diff BC Loss:  0.4358688294887543 \n",
      "PDE Loss:  4.631697654724121 \tPDE Loss:  6.248021602630615\n",
      "\n",
      "Iteration: 31500 \tTotal Loss: tensor(0.3153)\n",
      "\tphi IC Loss:  1.1820523468486499e-05 \tphi IC Loss Adaptive:  5.27808379047201e-06 \tu BC Loss:  0.002458084374666214 \tu diff BC Loss:  0.003467381466180086 \n",
      "PDE Loss:  0.2922918498516083 \tPDE Loss:  0.3844597041606903\n",
      "\n",
      "Iteration: 32000 \tTotal Loss: tensor(0.2799)\n",
      "\tphi IC Loss:  5.996597337798448e-06 \tphi IC Loss Adaptive:  2.7733891329262406e-05 \tu BC Loss:  0.0006845431635156274 \tu diff BC Loss:  0.0010296290274709463 \n",
      "PDE Loss:  0.24442514777183533 \tPDE Loss:  0.3558723032474518\n",
      "\n",
      "Iteration: 32500 \tTotal Loss: tensor(0.3491)\n",
      "\tphi IC Loss:  4.159819582127966e-05 \tphi IC Loss Adaptive:  6.313023186521605e-05 \tu BC Loss:  0.0005122320726513863 \tu diff BC Loss:  0.0019064592197537422 \n",
      "PDE Loss:  0.2419425994157791 \tPDE Loss:  0.3464663326740265\n",
      "\n",
      "Iteration: 33000 \tTotal Loss: tensor(5.7768)\n",
      "\tphi IC Loss:  0.0007331231026910245 \tphi IC Loss Adaptive:  0.0048006814904510975 \tu BC Loss:  0.0006827140459790826 \tu diff BC Loss:  0.0014146491885185242 \n",
      "PDE Loss:  0.2408982813358307 \tPDE Loss:  0.36031875014305115\n",
      "\n",
      "Iteration: 33500 \tTotal Loss: tensor(0.5015)\n",
      "\tphi IC Loss:  6.379768456099555e-05 \tphi IC Loss Adaptive:  0.0002206438803113997 \tu BC Loss:  0.0001148072988144122 \tu diff BC Loss:  0.0006183847435750067 \n",
      "PDE Loss:  0.21633416414260864 \tPDE Loss:  0.33414480090141296\n",
      "\n",
      "Iteration: 34000 \tTotal Loss: tensor(0.7288)\n",
      "\tphi IC Loss:  8.634898404125124e-05 \tphi IC Loss Adaptive:  0.00040405563777312636 \tu BC Loss:  9.422688890481368e-05 \tu diff BC Loss:  0.0013074381276965141 \n",
      "PDE Loss:  0.23700754344463348 \tPDE Loss:  0.3312668204307556\n",
      "\n",
      "Iteration: 34500 \tTotal Loss: tensor(0.8702)\n",
      "\tphi IC Loss:  0.00019152529421262443 \tphi IC Loss Adaptive:  0.00046254496555775404 \tu BC Loss:  7.6830227044411e-05 \tu diff BC Loss:  0.0006052268436178565 \n",
      "PDE Loss:  0.21549642086029053 \tPDE Loss:  0.3294725716114044\n",
      "\n",
      "Iteration: 35000 \tTotal Loss: tensor(0.2560)\n",
      "\tphi IC Loss:  1.2889621757494751e-05 \tphi IC Loss Adaptive:  3.4152501029893756e-05 \tu BC Loss:  3.96012983401306e-05 \tu diff BC Loss:  0.0005521835410036147 \n",
      "PDE Loss:  0.208347350358963 \tPDE Loss:  0.3264235258102417\n",
      "\n",
      "Iteration: 35500 \tTotal Loss: tensor(0.3276)\n",
      "\tphi IC Loss:  3.300595926702954e-05 \tphi IC Loss Adaptive:  5.9376056015025824e-05 \tu BC Loss:  2.1902777007198893e-05 \tu diff BC Loss:  0.001724134897813201 \n",
      "PDE Loss:  0.23344361782073975 \tPDE Loss:  0.3210428059101105\n",
      "\n",
      "Iteration: 36000 \tTotal Loss: tensor(1.6226)\n",
      "\tphi IC Loss:  0.00026626765611581504 \tphi IC Loss Adaptive:  0.0010610857279971242 \tu BC Loss:  0.0011857434874400496 \tu diff BC Loss:  0.015779828652739525 \n",
      "PDE Loss:  0.2782783508300781 \tPDE Loss:  0.334149569272995\n",
      "\n",
      "Iteration: 36500 \tTotal Loss: tensor(0.6198)\n",
      "\tphi IC Loss:  5.842466271133162e-05 \tphi IC Loss Adaptive:  0.00032399207702837884 \tu BC Loss:  7.189681491581723e-05 \tu diff BC Loss:  0.0014206892810761929 \n",
      "PDE Loss:  0.23593874275684357 \tPDE Loss:  0.3214341402053833\n",
      "\n",
      "Iteration: 37000 \tTotal Loss: tensor(0.2372)\n",
      "\tphi IC Loss:  1.8096692429026007e-06 \tphi IC Loss Adaptive:  2.872736104109208e-06 \tu BC Loss:  3.5557724913815036e-05 \tu diff BC Loss:  0.00103035441134125 \n",
      "PDE Loss:  0.2314942181110382 \tPDE Loss:  0.32357293367385864\n",
      "\n",
      "Iteration: 37500 \tTotal Loss: tensor(0.2875)\n",
      "\tphi IC Loss:  8.642829925520346e-06 \tphi IC Loss Adaptive:  5.183829489396885e-05 \tu BC Loss:  1.246974898094777e-05 \tu diff BC Loss:  0.00038963271072134376 \n",
      "PDE Loss:  0.22657068073749542 \tPDE Loss:  0.31283172965049744\n",
      "  *Saved ; Early Stopping for the min Bd Loss\n",
      "\n",
      "\n",
      "Iteration: 38000 \tTotal Loss: tensor(1.6669)\n",
      "\tphi IC Loss:  0.00048438322846777737 \tphi IC Loss Adaptive:  0.0009025917388498783 \tu BC Loss:  7.844247738830745e-05 \tu diff BC Loss:  0.003587608691304922 \n",
      "PDE Loss:  0.2762555181980133 \tPDE Loss:  0.3607995808124542\n",
      "\n",
      "Iteration: 38500 \tTotal Loss: tensor(0.2226)\n",
      "\tphi IC Loss:  2.0868631054327125e-06 \tphi IC Loss Adaptive:  3.5957887121185195e-06 \tu BC Loss:  2.4508353817509487e-05 \tu diff BC Loss:  7.613906927872449e-05 \n",
      "PDE Loss:  0.2167985439300537 \tPDE Loss:  0.3152252733707428\n",
      "\n",
      "Iteration: 39000 \tTotal Loss: tensor(0.4095)\n",
      "\tphi IC Loss:  4.0621322114020586e-05 \tphi IC Loss Adaptive:  0.00014634444960393012 \tu BC Loss:  4.460075433598831e-05 \tu diff BC Loss:  0.0007385480566881597 \n",
      "PDE Loss:  0.2217794805765152 \tPDE Loss:  0.31692129373550415\n",
      "\n",
      "Iteration: 39500 \tTotal Loss: tensor(0.5307)\n",
      "\tphi IC Loss:  8.231651736423373e-05 \tphi IC Loss Adaptive:  0.00022725513554178178 \tu BC Loss:  0.00012067126954207197 \tu diff BC Loss:  0.0011075306683778763 \n",
      "PDE Loss:  0.21985015273094177 \tPDE Loss:  0.31701406836509705\n",
      "\n",
      "Iteration: 40000 \tTotal Loss: tensor(3.0960)\n",
      "\tphi IC Loss:  0.0007612984045408666 \tphi IC Loss Adaptive:  0.0019266476156190038 \tu BC Loss:  0.0017563337460160255 \tu diff BC Loss:  0.0174440685659647 \n",
      "PDE Loss:  0.3888201117515564 \tPDE Loss:  0.5252148509025574\n",
      "\n",
      "Iteration: 40500 \tTotal Loss: tensor(0.3530)\n",
      "\tphi IC Loss:  2.5517731046420522e-05 \tphi IC Loss Adaptive:  9.764811693457887e-05 \tu BC Loss:  2.612426396808587e-05 \tu diff BC Loss:  0.0019484978402033448 \n",
      "PDE Loss:  0.22789645195007324 \tPDE Loss:  0.3214441239833832\n",
      "\n",
      "Iteration: 41000 \tTotal Loss: tensor(0.5048)\n",
      "\tphi IC Loss:  6.888429197715595e-05 \tphi IC Loss Adaptive:  0.0001987085270229727 \tu BC Loss:  8.017119398573413e-06 \tu diff BC Loss:  0.0007476042956113815 \n",
      "PDE Loss:  0.2364467978477478 \tPDE Loss:  0.320720374584198\n",
      "  *Saved ; Early Stopping for the min Bd Loss\n",
      "\n",
      "\n",
      "Iteration: 41500 \tTotal Loss: tensor(0.2337)\n",
      "\tphi IC Loss:  2.409394710412016e-06 \tphi IC Loss Adaptive:  6.066095465939725e-06 \tu BC Loss:  3.7708348827436566e-05 \tu diff BC Loss:  0.0008412700844928622 \n",
      "PDE Loss:  0.22435614466667175 \tPDE Loss:  0.3258350193500519\n",
      "\n",
      "Iteration: 42000 \tTotal Loss: tensor(0.3062)\n",
      "\tphi IC Loss:  1.548537147755269e-05 \tphi IC Loss Adaptive:  7.739911234239116e-05 \tu BC Loss:  5.037055962020531e-05 \tu diff BC Loss:  0.0010936034377664328 \n",
      "PDE Loss:  0.21219153702259064 \tPDE Loss:  0.32309019565582275\n",
      "\n",
      "Iteration: 42500 \tTotal Loss: tensor(0.2316)\n",
      "\tphi IC Loss:  4.894153335044393e-06 \tphi IC Loss Adaptive:  1.1790876669692807e-05 \tu BC Loss:  2.039526225416921e-05 \tu diff BC Loss:  0.00026067945873364806 \n",
      "PDE Loss:  0.21466629207134247 \tPDE Loss:  0.31144025921821594\n",
      "\n",
      "Iteration: 43000 \tTotal Loss: tensor(0.2821)\n",
      "\tphi IC Loss:  1.0708189620345365e-05 \tphi IC Loss Adaptive:  5.453238554764539e-05 \tu BC Loss:  2.2222729967324995e-05 \tu diff BC Loss:  0.0002453291672281921 \n",
      "PDE Loss:  0.21662122011184692 \tPDE Loss:  0.30696389079093933\n",
      "\n",
      "Iteration: 43500 \tTotal Loss: tensor(0.2287)\n",
      "\tphi IC Loss:  2.6056177375721745e-06 \tphi IC Loss Adaptive:  5.167466042621527e-06 \tu BC Loss:  9.430164936929941e-06 \tu diff BC Loss:  0.0001764850167091936 \n",
      "PDE Loss:  0.22073937952518463 \tPDE Loss:  0.3115818500518799\n",
      "\n",
      "Iteration: 44000 \tTotal Loss: tensor(0.2202)\n",
      "\tphi IC Loss:  2.0358640995254973e-06 \tphi IC Loss Adaptive:  1.3966921414976241e-06 \tu BC Loss:  2.723498073464725e-05 \tu diff BC Loss:  0.0004187242011539638 \n",
      "PDE Loss:  0.21636727452278137 \tPDE Loss:  0.3121803402900696\n",
      "  *Saved ; Early Stopping for the min IC Loss\n",
      "\n",
      "\n",
      "Iteration: 44500 \tTotal Loss: tensor(0.4813)\n",
      "\tphi IC Loss:  8.287635864689946e-05 \tphi IC Loss Adaptive:  0.00016920006601139903 \tu BC Loss:  2.5324134185211733e-05 \tu diff BC Loss:  0.007682084105908871 \n",
      "PDE Loss:  0.22153286635875702 \tPDE Loss:  0.32040444016456604\n",
      "\n",
      "Iteration: 45000 \tTotal Loss: tensor(1.1530)\n",
      "\tphi IC Loss:  0.00024523792671971023 \tphi IC Loss Adaptive:  0.0006708764121867716 \tu BC Loss:  1.795970820239745e-05 \tu diff BC Loss:  0.002312316792085767 \n",
      "PDE Loss:  0.2345651537179947 \tPDE Loss:  0.32731616497039795\n",
      "\n",
      "Iteration: 45500 \tTotal Loss: tensor(0.4402)\n",
      "\tphi IC Loss:  7.089635619195178e-05 \tphi IC Loss Adaptive:  0.00015005050227046013 \tu BC Loss:  2.4965154807432555e-05 \tu diff BC Loss:  0.0002254158753203228 \n",
      "PDE Loss:  0.21897026896476746 \tPDE Loss:  0.3138819932937622\n",
      "\n",
      "Iteration: 46000 \tTotal Loss: tensor(0.6122)\n",
      "\tphi IC Loss:  0.00012828617764171213 \tphi IC Loss Adaptive:  0.00028886328800581396 \tu BC Loss:  8.193315443350002e-05 \tu diff BC Loss:  0.0006485373014584184 \n",
      "PDE Loss:  0.19429010152816772 \tPDE Loss:  0.31001150608062744\n",
      "\n",
      "Iteration: 46500 \tTotal Loss: tensor(1.2794)\n",
      "\tphi IC Loss:  0.000249332602834329 \tphi IC Loss Adaptive:  0.0007953341118991375 \tu BC Loss:  0.00028774814563803375 \tu diff BC Loss:  0.007458931300789118 \n",
      "PDE Loss:  0.22699162364006042 \tPDE Loss:  0.3339381814002991\n",
      "\n",
      "Iteration: 47000 \tTotal Loss: tensor(0.2134)\n",
      "\tphi IC Loss:  1.676597207733721e-06 \tphi IC Loss Adaptive:  4.240160706103779e-06 \tu BC Loss:  2.085143387375865e-05 \tu diff BC Loss:  0.00011560926213860512 \n",
      "PDE Loss:  0.20729809999465942 \tPDE Loss:  0.31381264328956604\n",
      "  *Saved ; Early Stopping for the Minimal Total Loss\n",
      "\n",
      "\n",
      "Iteration: 47500 \tTotal Loss: tensor(0.3989)\n",
      "\tphi IC Loss:  7.257301331264898e-05 \tphi IC Loss Adaptive:  0.000105606100987643 \tu BC Loss:  5.372650775825605e-05 \tu diff BC Loss:  0.0020856682676821947 \n",
      "PDE Loss:  0.21856217086315155 \tPDE Loss:  0.30752065777778625\n",
      "\n",
      "Iteration: 48000 \tTotal Loss: tensor(0.2793)\n",
      "\tphi IC Loss:  1.381313632009551e-05 \tphi IC Loss Adaptive:  4.5455781219061464e-05 \tu BC Loss:  0.0001392328122165054 \tu diff BC Loss:  9.95238806353882e-05 \n",
      "PDE Loss:  0.21982821822166443 \tPDE Loss:  0.3082183599472046\n",
      "\n",
      "Iteration: 48500 \tTotal Loss: tensor(6.6328)\n",
      "\tphi IC Loss:  0.001277682138606906 \tphi IC Loss Adaptive:  0.005024983547627926 \tu BC Loss:  0.0020552645437419415 \tu diff BC Loss:  0.0019671539776027203 \n",
      "PDE Loss:  0.3260778784751892 \tPDE Loss:  0.47263285517692566\n",
      "\n",
      "Iteration: 49000 \tTotal Loss: tensor(0.2183)\n",
      "\tphi IC Loss:  2.9733603241766104e-06 \tphi IC Loss Adaptive:  6.411514277715469e-06 \tu BC Loss:  6.16709585301578e-05 \tu diff BC Loss:  0.00010885934170801193 \n",
      "PDE Loss:  0.20877385139465332 \tPDE Loss:  0.30350828170776367\n",
      "\n",
      "Iteration: 49500 \tTotal Loss: tensor(0.2461)\n",
      "\tphi IC Loss:  8.584090210206341e-06 \tphi IC Loss Adaptive:  2.193297586927656e-05 \tu BC Loss:  2.0141684217378497e-05 \tu diff BC Loss:  0.0008212526445277035 \n",
      "PDE Loss:  0.21478958427906036 \tPDE Loss:  0.31407803297042847\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "currentDateTime = datetime.now()\n",
    "print(\"Date of Today : \", currentDateTime.month, \" /\", currentDateTime.day, \"\\nHour : \", currentDateTime.hour) \n",
    "\n",
    "#Date of Today\n",
    "ctime = f\"{currentDateTime.month}_{currentDateTime.day}_{currentDateTime.hour}h\"\n",
    "#torch.autograd.detect_anomaly(check_nan=True)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Choose Test Problem\n",
    "from BM_WZ_CHnet import BM_WZ_CHnet\n",
    "Net = BM_WZ_CHnet\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#Create and Train the network\n",
    "def create_network(layers, preload = False, preload_name=''):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    net = Net(layers)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    if preload == True:\n",
    "        try:\n",
    "            net.load_state_dict(torch.load(preload_name, map_location=torch.device('cpu')))\n",
    "            print('Loaded Successfully')\n",
    "        except Exception as error:\n",
    "            print('Loading Failed: ', error)\n",
    "            pass\n",
    "        \n",
    "    time_vec = [0, 0, 0, 0]\n",
    "    \n",
    "    #Set final times for running training_loop\n",
    "    time_slices = np.array([1])\n",
    "    \n",
    "    global epsilon #used to track loss\n",
    "    epsilon = []\n",
    "    global a_u_set\n",
    "    a_u_set = []\n",
    "    global a_u_diff_set\n",
    "    a_u_diff_set = []\n",
    "    \n",
    "    global b_u_set\n",
    "    b_u_set = []\n",
    "    global c_ac_set\n",
    "    c_ac_set = []\n",
    "    \n",
    "    a_u = 1 ##10 \n",
    "    a_u_diff = 1\n",
    "    \n",
    "    b_u = 1000 ##100 \n",
    "    c_ac = 1 ###1/10 ##1000\n",
    "    \n",
    "    \n",
    "    print('Training PDE')\n",
    "    \n",
    "    iteration_vec = [120000]\n",
    "    learning_rate_vec = 1e-3 * np.ones_like(iteration_vec)\n",
    "    #r_vec = [30, 20, 10, 10]\n",
    "    \n",
    "    \n",
    "    for i in range(len(iteration_vec)):\n",
    "        #Set loop to optimize in progressively smaller learning rates\n",
    "        print(f'Executing Pass {i+1}')\n",
    "        iterations = iteration_vec[i]\n",
    "        learning_rate = learning_rate_vec[i]   \n",
    "        #r = r_vec[i] #determines sampling distribution decay rate\n",
    "        r=1 \n",
    "        \n",
    "        training_loop(net, time_slices, iterations, learning_rate, r, record_loss = 100, print_loss = 500, a_u = a_u, a_u_diff = a_u_diff, b_u = b_u, c_ac = c_ac )\n",
    "        torch.save(net.state_dict(), f\"CH_Benchmarks_Pass_{i+1}.pt\")\n",
    "        \n",
    "        time_slices = [time_slices[-1]]\n",
    "        \n",
    "        np.savetxt('epsilon.txt', epsilon)\n",
    "        time_vec[i] = time.time()\n",
    "\n",
    "\n",
    "    np.savetxt('epsilon.txt', epsilon)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Total Time:\\t\", end-start)\n",
    "    for i in range(len(iteration_vec)):\n",
    "        if i>0:\n",
    "            print(f'Pass {i+1} Time: {time_vec[i] - time_vec[i-1]}')\n",
    "        else:\n",
    "            print(f'Pass 1 Time: {time_vec[0] - start}')\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def exponential_time_sample(collocation, t_l, t_u, r):\n",
    "    t_pre = np.random.uniform(0,1, size=(collocation, 1))\n",
    "    \n",
    "    t = -np.log(1-t_pre+t_pre*np.exp(-r*(t_u-t_l)))/r\n",
    "    \n",
    "    return t\n",
    "\n",
    "def initialize_adaptive_x(net, x_l, x_u, size_IC, size_domain):\n",
    "    total_size = size_IC + size_domain\n",
    "    #Initialize\n",
    "    x = np.random.uniform(low=x_l, high=x_u, size=(total_size,1))\n",
    "    #y = np.random.uniform(low=y_l, high=y_u, size=(total_size,1))\n",
    "    \n",
    "    its = 10000\n",
    "    for i in range(its):\n",
    "        x_pt = Variable(torch.from_numpy(x).float(), requires_grad=True).to(device)\n",
    "        #y_pt = Variable(torch.from_numpy(y).float(), requires_grad=True).to(device)\n",
    "        \n",
    "        psi = net.W_exact(x_pt) #calculate density (scaled by constant)\n",
    "        psi = psi.detach().cpu().numpy()\n",
    "        \n",
    "        #First Proposal\n",
    "        x_prop = np.random.uniform(low=x_l, high=x_u, size=(total_size,1))\n",
    "        #y_prop = np.random.uniform(low=y_l, high=y_u, size=(total_size,1))\n",
    "        \n",
    "        x_prop_pt = Variable(torch.from_numpy(x_prop).float(), requires_grad=True).to(device)\n",
    "        #y_prop_pt = Variable(torch.from_numpy(y_prop).float(), requires_grad=True).to(device)\n",
    "        psi_prop = net.W_exact(x_prop_pt) #calculate density (scaled by constant)\n",
    "        psi_prop = psi_prop.detach().cpu().numpy()\n",
    "        \n",
    "        a = psi_prop/psi\n",
    "        \n",
    "        #accept worse proposals with probability a\n",
    "        rand = np.random.uniform(low=0, high=1, size=(total_size,1))\n",
    "        x[rand<a] = x_prop[rand<a]\n",
    "        #y[rand<a] = y_prop[rand<a]\n",
    "\n",
    "    x_IC, x_dom = np.split(x, [size_IC])\n",
    "    #y_IC, y_dom = np.split(y, [size_IC])\n",
    "    return x_IC, x_dom\n",
    "\n",
    "def step_adaptive_x(net, x_IC, x_dom, t_dom, x_l, x_u):\n",
    "    x_IC = x_IC.detach()\n",
    "    #y_IC = y_IC.detach()\n",
    "    x_dom = x_dom.detach() \n",
    "    #y_dom = y_dom.detach()\n",
    "    t_dom = t_dom.detach()\n",
    "    \n",
    "    Psi_IC = net.W_exact(x_IC)\n",
    "    Psi_dom = net.W(x_dom, t_dom)\n",
    "    \n",
    "    its = 1\n",
    "    for i in range(its):    \n",
    "        #First Proposal\n",
    "        x_prop_IC = (x_u-x_l)*torch.rand_like(x_IC) + x_l\n",
    "        #y_prop_IC = (y_u-y_l)*torch.rand_like(y_IC) + y_l\n",
    "        \n",
    "        x_prop_dom = (x_u-x_l)*torch.rand_like(x_dom) + x_l\n",
    "        #y_prop_dom = (y_u-y_l)*torch.rand_like(y_dom) + y_l\n",
    "        \n",
    "        psi_prop_IC = net.W_exact(x_prop_IC) #calculate density (scaled by constant)\n",
    "        psi_prop_dom = net.W(x_prop_dom, t_dom)\n",
    "        \n",
    "        a_IC = psi_prop_IC/Psi_IC\n",
    "        a_dom = psi_prop_dom/Psi_dom\n",
    "        \n",
    "        #accept worse proposals with probability a\n",
    "        rand_IC = torch.rand_like(x_IC)\n",
    "        rand_dom = torch.rand_like(x_dom)\n",
    "        \n",
    "        x_IC[rand_IC<a_IC] = x_prop_IC[rand_IC<a_IC]\n",
    "        #y_IC[rand_IC<a_IC] = y_prop_IC[rand_IC<a_IC]\n",
    "        \n",
    "        x_dom[rand_dom<a_dom] = x_prop_dom[rand_dom<a_dom]\n",
    "        #y_dom[rand_dom<a_dom] = y_prop_dom[rand_dom<a_dom]\n",
    "\n",
    "    return x_IC, x_dom\n",
    "\n",
    "def autograd_W(net, input):\n",
    "    list_grad = []\n",
    "    layers = [[net.layers_u[0].weight,net.layers_u[1].weight,net.layers_u[2].weight,net.layers_u[3].weight, net.layers_u[4].weight,net.layers_u[5].weight,net.layers_u[6].weight]]\n",
    "    #[net.hidden_layer1_u1.weight,net.hidden_layer2_u1.weight,net.hidden_layer3_u1.weight, net.hidden_layer4_u1.weight,net.output_layer_u1.weight],\n",
    "                #[net.hidden_layer1_u2.weight,net.hidden_layer2_u2.weight,net.hidden_layer3_u2.weight, net.hidden_layer4_u2.weight,net.output_layer_u2.weight], \n",
    "                 #[net.hidden_layer1_P.weight,net.hidden_layer2_P.weight,net.hidden_layer3_P.weight, net.hidden_layer4_P.weight,net.output_layer_P.weight],\n",
    "                  \n",
    "    for j in range(0, len(layers)):\n",
    "        for i in range(0,7):\n",
    "            diff = layers[j][i]\n",
    "            try:\n",
    "                result = torch.autograd.grad(input.sum(), diff, create_graph=True, retain_graph=True)[0]\n",
    "                list_grad.append(result)\n",
    "            except:\n",
    "                pass\n",
    "    return list_grad\n",
    "\n",
    "def training_loop(net, time_slices, iterations, learning_rate, r, record_loss, print_loss, a_u, a_u_diff, b_u, c_ac):\n",
    "    global epsilon #used to track and record loss\n",
    "    global a_u_set\n",
    "    global a_u_diff_set\n",
    "    global b_u_set\n",
    "    global c_ac_set\n",
    "    \n",
    "    # Domain boundary\n",
    "    x_l = net.x1_l\n",
    "    x_u = net.x1_u\n",
    "    \n",
    "    #y_l = net.x2_l\n",
    "    #y_u = net.x2_u\n",
    "\n",
    "    #time starts at lower bound 0, ends at upper bouund updated in slices\n",
    "    t_l = 0\n",
    "\n",
    "    # Generate following random numbers for x, y t\n",
    "    BC_collocation = int(160) #1000\n",
    "    IC_collocation = int(100) #5000\n",
    "    pde_collocation = int(2000) #10000\n",
    "    \n",
    "    IC_collocation_adaptive = int(40) #5000\n",
    "    pde_collocation_adaptive = int(8000) #10000\n",
    "    lamba = .9\n",
    "    #learning rate update\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    for final_time in time_slices:\n",
    "        min_loss = 20\n",
    "        min_mse_BC = .01\n",
    "        \n",
    "        min_mse_AC = .01\n",
    "        min_mse_IC = .01\n",
    "        \n",
    "        epoch = 0\n",
    "         \n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"Current Final Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        \n",
    "        #Initialize Adaptive Sampling points outside the loop\n",
    "        ##Define Colloacation Points with Initial Condition\n",
    "        \n",
    "        x_IC_adaptive, x_domain_adaptive = initialize_adaptive_x(net, x_l, x_u, IC_collocation_adaptive, pde_collocation_adaptive)\n",
    "\n",
    "        input_x_IC_adaptive = Variable(torch.from_numpy(x_IC_adaptive).float(), requires_grad=True).to(device)\n",
    "        #input_y_IC_adaptive = Variable(torch.from_numpy(y_IC_adaptive).float(), requires_grad=True).to(device)\n",
    "\n",
    "        #Define Inner Domain Collocation Points\n",
    "        t_domain_adaptive = exponential_time_sample(pde_collocation_adaptive, t_l, final_time, r)\n",
    "\n",
    "        input_x_domain_adaptive = Variable(torch.from_numpy(x_domain_adaptive).float(), requires_grad=True).to(device)\n",
    "        #input_y_domain_adaptive = Variable(torch.from_numpy(y_domain_adaptive).float(), requires_grad=True).to(device)\n",
    "        input_t_domain_adaptive = Variable(torch.from_numpy(t_domain_adaptive).float(), requires_grad=True).to(device)\n",
    "        \n",
    "        #grad_NS_loss_mean = 0.1 **5\n",
    "        #grad_NS_div_loss_mean = 0.1 **5\n",
    "        grad_AC_loss_mean = 1\n",
    "        \n",
    "        grad_BC_u_loss_mean = 1\n",
    "        grad_BC_u_diff_loss_mean = 1\n",
    "        grad_IC_u_loss_mean = 1000\n",
    "        \n",
    "        \n",
    "        epoch = 0\n",
    "        while epoch <= iterations:\n",
    "            # Resetting gradients to zero            \n",
    "            net.optimizer.zero_grad()    \n",
    "            \n",
    "            if epoch > 0:\n",
    "                input_x_IC_adaptive, input_x_domain_adaptive = step_adaptive_x(net, input_x_IC_adaptive, input_x_domain_adaptive, input_t_domain_adaptive, x_l, x_u)\n",
    "                \n",
    "                input_x_IC_adaptive = Variable(input_x_IC_adaptive, requires_grad=True).to(device)\n",
    "                #input_y_IC_adaptive = Variable(input_y_IC_adaptive, requires_grad=True).to(device)\n",
    "                \n",
    "                input_x_domain_adaptive = Variable(input_x_domain_adaptive, requires_grad=True).to(device)\n",
    "                #input_y_domain_adaptive = Variable(input_y_domain_adaptive, requires_grad=True).to(device)\n",
    "                input_t_domain_adaptive = Variable(input_t_domain_adaptive, requires_grad=True).to(device)\n",
    "                \n",
    "            ##Define Colloacation Points with Initial Condition\n",
    "            x_IC = np.random.uniform(low=x_l, high=x_u, size=(IC_collocation,1))\n",
    "            #y_IC = np.random.uniform(low=y_l, high=y_u, size=(IC_collocation,1))\n",
    "\n",
    "            input_x_IC = Variable(torch.from_numpy(x_IC).float(), requires_grad=True).to(device)\n",
    "            #input_y_IC = Variable(torch.from_numpy(y_IC).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            ##Define Boundary Condition Collocation Points\n",
    "            x_BC = np.random.uniform(low=x_l, high=x_u, size=(BC_collocation,1))\n",
    "            #y_BC = np.random.uniform(low=y_l, high=y_u, size=(BC_collocation,1))       \n",
    "            t_BC = np.random.uniform(low=t_l, high=final_time, size=(BC_collocation,1))\n",
    "    \n",
    "            input_x_BC= Variable(torch.from_numpy(x_BC).float(), requires_grad=True).to(device)\n",
    "            #input_y_BC = Variable(torch.from_numpy(y_BC).float(), requires_grad=True).to(device)\n",
    "            input_t_BC = Variable(torch.from_numpy(t_BC).float(), requires_grad=True).to(device)\n",
    "    \n",
    "            #Define Inner Domain Collocation Points\n",
    "            x_domain = np.random.uniform(low= x_l, high=x_u, size=(pde_collocation, 1))\n",
    "            #y_domain = np.random.uniform(low= y_l, high=y_u, size=(pde_collocation, 1))\n",
    "            t_domain = exponential_time_sample(pde_collocation, t_l, final_time, r)\n",
    "            #t_domain = np.random.uniform(low= t_l, high=final_time, size=(pde_collocation, 1)) \n",
    "    \n",
    "            input_x_domain = Variable(torch.from_numpy(x_domain).float(), requires_grad=True).to(device)\n",
    "            #input_y_domain = Variable(torch.from_numpy(y_domain).float(), requires_grad=True).to(device)\n",
    "            input_t_domain = Variable(torch.from_numpy(t_domain).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            ### Training steps\n",
    "            ## Compute Loss\n",
    "            # Loss based on Initial Condition\n",
    "            mse_IC = net.Initial_Condition_Loss(input_x_IC)\n",
    "            mse_IC_adaptive = net.Initial_Condition_Loss(input_x_IC_adaptive)\n",
    "            \n",
    "            mse_IC_total = mse_IC + mse_IC_adaptive\n",
    "        \n",
    "            # Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "            mse_BC_u, mse_BC_u_diff = net.Boundary_Loss(input_x_BC, input_t_BC)\n",
    "        \n",
    "            # Loss based on PDE\n",
    "            mse_PDE = net.PDE_Loss(input_x_domain, input_t_domain)\n",
    "            mse_PDE_adaptive = net.PDE_Loss(input_x_domain_adaptive, input_t_domain_adaptive)\n",
    "            \n",
    "            mse_PDE_total = mse_PDE #+ mse_PDE_adaptive\n",
    "            \n",
    "            # Sum Loss\n",
    "            loss = b_u * mse_IC_total + a_u *mse_BC_u+ a_u_diff * mse_BC_u_diff + c_ac * mse_PDE_total\n",
    "            \n",
    "            '''\n",
    "            if epoch%1 == 0:\n",
    "                \n",
    "                #compute next step\n",
    "                \n",
    "                if c_ac < 5000 :\n",
    "                    max_set = []\n",
    "                    for e in autograd_W(net, mse_PDE_total):\n",
    "                        m = torch.max(torch.abs(e))\n",
    "                        max_set.append(m.item())\n",
    "                    grad_AC_loss_mean = np.max(max_set)\n",
    "                \n",
    "                if a_u < 5000 :\n",
    "                    mean_set = []\n",
    "                    for e in autograd_W(net, mse_BC_u):\n",
    "                        m = torch.mean(torch.abs(e))\n",
    "                        mean_set.append(m.item())\n",
    "                    grad_BC_u_loss_mean = np.mean(mean_set)\n",
    "                \n",
    "                if a_u < 5000 :\n",
    "                    mean_set = []\n",
    "                    for e in autograd_W(net, mse_BC_u_diff):\n",
    "                        m = torch.mean(torch.abs(e))\n",
    "                        mean_set.append(m.item())\n",
    "                    grad_BC_u_diff_loss_mean = np.mean(mean_set)\n",
    "                \n",
    "                if b_u < 5000 :\n",
    "                    mean_set = []\n",
    "                    for e in autograd_W(net, mse_IC_total):\n",
    "                        m = torch.mean(torch.abs(e))\n",
    "                        mean_set.append(m.item())\n",
    "                    grad_IC_u_loss_mean = np.mean(mean_set)\n",
    "                \n",
    "                #grad_mean_sum = (grad_BC_u_loss_mean + grad_BC_u_diff_loss_mean + grad_IC_u_loss_mean+ grad_AC_loss_mean )\n",
    "                \n",
    "                \n",
    "                # coeffiecient of BC => a, coeffiecient of IC => b\n",
    "                a_u_next = (grad_AC_loss_mean)/(grad_BC_u_loss_mean)\n",
    "                a_u_diff_next = (grad_AC_loss_mean)/(grad_BC_u_diff_loss_mean)\n",
    "                \n",
    "                b_u_next = (grad_AC_loss_mean)/(grad_IC_u_loss_mean)\n",
    "                \n",
    "                #c_ac_next = (grad_mean_sum)/(grad_AC_loss_mean)\n",
    "                \n",
    "                # #update rates\n",
    "                if a_u_next < 5000:\n",
    "                    a_u = (1-lamba) * a_u + lamba * a_u_next\n",
    "                if a_u_diff_next < 5000:\n",
    "                    a_u_diff = (1-lamba) * a_u_diff + lamba * a_u_diff_next\n",
    "                \n",
    "                if b_u_next < 5000:\n",
    "                    b_u = (1-lamba) * b_u + lamba * b_u_next\n",
    "                #if c_ac_next < 5000:\n",
    "                    #c_ac = (1-lamba) * c_ac + lamba * c_ac_next\n",
    "            '''\n",
    "            \n",
    "            ## Compute Gradients\n",
    "            # Compute Actual Gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            #Gradient Norm Clipping\n",
    "            #torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm = 1000, error_if_nonfinite=False) #note max norm is more art than science\n",
    "            \n",
    "            def closure():\n",
    "                return loss\n",
    "            \n",
    "            ## Step Optimizer\n",
    "            net.optimizer.step(closure)\n",
    "            #net.optimizer.step()\n",
    "            \n",
    "            ### Update Epoch\n",
    "            epoch = epoch + 1\n",
    "            \n",
    "            ### Save and Print\n",
    "            \n",
    "            # if epoch < 10:\n",
    "            #     #save adaptive points\n",
    "            #     torch.save(input_x_IC_adaptive, f'x_IC_adaptive_{epoch}.pt')\n",
    "            #     torch.save(input_y_IC_adaptive, f'y_IC_adaptive_{epoch}.pt')\n",
    "            #     torch.save(input_x_domain_adaptive, f'x_dom_adaptive_{epoch}.pt')\n",
    "            #     torch.save(input_y_domain_adaptive, f'y_dom_adaptive_{epoch}.pt')\n",
    "            #     torch.save(input_t_domain_adaptive, f't_dom_adaptive_{epoch}.pt')\n",
    "            #Print Loss every 1000 Epochs\n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch == 1 or epoch%record_loss == 0:\n",
    "                    epsilon = np.append(epsilon, loss.cpu().detach().numpy())\n",
    "                    #a_u_set = np.append(a_u_set, a_u)\n",
    "                    #a_u_diff_set = np.append(a_u_diff_set, a_u_diff)\n",
    "                    #b_u_set = np.append(b_u_set, b_u)\n",
    "                    #c_ac_set = np.append(c_ac_set, c_ac)\n",
    "                if epoch == 1 or epoch%print_loss == 0:\n",
    "                    print(\"\\nIteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    print(\"\\tphi IC Loss: \", mse_IC.item(),\"\\tphi IC Loss Adaptive: \", mse_IC_adaptive.item(),\"\\tu BC Loss: \", mse_BC_u.item(), \n",
    "                          \"\\tu diff BC Loss: \", mse_BC_u_diff.item(), \"\\nPDE Loss: \", mse_PDE.item(), \"\\tPDE Loss: \", mse_PDE_adaptive.item())\n",
    "                    \n",
    "                    if mse_PDE_total.cpu().detach().numpy() < min_mse_AC:\n",
    "                        min_mse_AC = mse_PDE_total.cpu().detach().numpy()\n",
    "                        torch.save(net.state_dict(), f\"ES_min_CHloss_lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                        print('  *Saved ; Early Stopping for the min CH PDE Loss\\n')\n",
    "                    if mse_BC_u.cpu().detach().numpy() < min_mse_BC:\n",
    "                        min_mse_BC = mse_BC_u.cpu().detach().numpy()\n",
    "                        torch.save(net.state_dict(), f\"ES_min_mse_Bd_lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                        print('  *Saved ; Early Stopping for the min Bd Loss\\n')\n",
    "                    if mse_IC_total.cpu().detach().numpy() < min_mse_IC:\n",
    "                        min_mse_IC = mse_IC_total.cpu().detach().numpy()\n",
    "                        torch.save(net.state_dict(), f\"ES_min_mse_IC_lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                        print('  *Saved ; Early Stopping for the min IC Loss\\n')\n",
    "                    if loss.cpu().detach().numpy() < min_loss:\n",
    "                        min_loss = loss.cpu().detach().numpy()\n",
    "                        torch.save(net.state_dict(), f\"ES_min_loss_lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                        print('  *Saved ; Early Stopping for the Minimal Total Loss\\n')\n",
    "                if epoch%6000 == 0:\n",
    "                    np.savetxt(f\"{ctime}epsilon.txt\", epsilon)\n",
    "                    #np.savetxt(f\"{ctime}_a_u.txt\", a_u_set)\n",
    "                    #np.savetxt(f\"{ctime}_a_u_diff.txt\", a_u_diff_set)\n",
    "                    #np.savetxt(f\"{ctime}_b_u.txt\", b_u_set)\n",
    "                    #np.savetxt(f\"{ctime}_c_ac.txt\", c_ac_set)\n",
    "                    torch.save(net.state_dict(), f\"lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "                    \n",
    "    \n",
    "layers=[2, 128, 128, 128, 128, 128, 128, 1]\n",
    "create_network(layers, preload=True, preload_name='lr0.001_t0.2_3_4_15h.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76516c98-2e4a-4c24-85a9-06934d6b6b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
