{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4be94-868d-4299-9e65-5251b94fce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of Today :  4  / 3 \n",
      "Hour :  18\n",
      "Training PDE\n",
      "\n",
      "\n",
      "Training Pass 1\n",
      "\n",
      "\n",
      "Current End Time: 0.1 Current Learning Rate:  0.001\n",
      "\n",
      "Iteration: 1 \tTotal Loss: tensor(1.8534)\n",
      "\tIC Loss:  0.8708339929580688 \t BC u Loss:  0.47816452383995056 \t BC u_x Loss:  0.003568673739209771 \n",
      "AC PDE Loss:  0.5008800029754639 \tadaptive PDE Loss:  5.005212783813477\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from scipy.stats import qmc\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from Building_Net import Net\n",
    "from Conditions import lossIC, lossBdry,lossNSpde, lossNSpde_rank\n",
    "\n",
    "currentDateTime = datetime.now()\n",
    "print(\"Date of Today : \", currentDateTime.month, \" /\", currentDateTime.day, \"\\nHour : \", currentDateTime.hour) \n",
    "ctime = f\"{currentDateTime.month}_{currentDateTime.day}_{currentDateTime.hour}h\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_network(load, loadfile):\n",
    "    \n",
    "    net = Net()\n",
    "    net = net.to(device)\n",
    "    epsilon = []\n",
    "    #Attempt to load the saved pt. file\n",
    "    if load == True:\n",
    "        try:\n",
    "            net.load_state_dict(torch.load(loadfile, map_location=torch.device(device)))\n",
    "        except:\n",
    "            print(\"\\nLoading file was failed\\n\")\n",
    "        else:\n",
    "            print(\"\\nLoading file was completed\\n\")\n",
    "    \n",
    "    print('Training PDE')\n",
    "    start = time.time() #initialize tracking computational time\n",
    "    \n",
    "    partial_time_set = [0, 0, 0] #initialize time recording list with number of learnning rates on whole domain training\n",
    "    \n",
    "    for i in range(len(partial_time_set)):\n",
    "        if i == 0:\n",
    "            #First loop uses progressively increasing time intervals\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            time_slices = np.array([.1, .2, .3]) #, .4, .5, .6, .7, .8, .9, 1\n",
    "            iterations = 100 #iterations for each learning rate\n",
    "            learning_rate = 10**-3\n",
    "        elif i == 1:\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            #time_slices = [time_slices[-1]]\n",
    "            time_slices = [time_slices[-1]]\n",
    "            iterations = 0 #iterations for each learning rate\n",
    "            learning_rate = 10**-4\n",
    "        elif i == 2:\n",
    "            print(f'\\n\\nTraining Pass {i+1}')\n",
    "            time_slices = [time_slices[-1]]\n",
    "            iterations = 0 #iterations for each learning rate\n",
    "            learning_rate = 10**-4\n",
    "        \n",
    "        training_loop(net, time_slices, iterations, learning_rate, record_loss = 100, print_loss = 100, epsilon = epsilon)\n",
    "        torch.save(net.state_dict(), f\"{ctime}_Training_{i+1}.pt\")\n",
    "        partial_time_set[i] = time.time()\n",
    "        np.savetxt(f\"{ctime}epsilon_{i}.txt\", epsilon)\n",
    "\n",
    "    print(\"Total Time:\\t\", partial_time_set[-1]-start, '\\nPass 1 Time:\\t', partial_time_set[0]-start, \n",
    "          '\\nPass 2 Time:\\t', partial_time_set[1]-partial_time_set[0], '\\nPass 3 Time:\\t', partial_time_set[2]-partial_time_set[1])\n",
    "   \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def exp_time_sample(collocation_pt_number, t_l, t_u, r):\n",
    "    t_uni = np.random.uniform(0,1, size=(collocation_pt_number, 1))\n",
    "    \n",
    "    t = -np.log(1-t_uni+t_uni*np.exp(-r*(t_u-t_l)))/r\n",
    "    \n",
    "    return t\n",
    "def training_loop(net, time_slices, iterations, learning_rate, record_loss, print_loss, epsilon):\n",
    "    \n",
    "    # Domain boundary values\n",
    "    x_l = net.x1_l\n",
    "    x_u = net.x1_u\n",
    "    \n",
    "    #time starts at 0, ends at upper bouund updated in time_slices\n",
    "    t_l = 0\n",
    "\n",
    "    #numbers of sampling collocation points on each part\n",
    "    IC_collocation = int(512)\n",
    "    BC_collocation = int(200)\n",
    "    pde_collocation = int(10000)\n",
    "\n",
    "    #sampler setting for LatinHypercube sampling\n",
    "    IC_lh_sampler = qmc.LatinHypercube(d=1)\n",
    "    BC_lh_sampler = qmc.LatinHypercube(d=1)\n",
    "    PDE_lh_sampler = qmc.LatinHypercube(d=2)\n",
    "    PDE_lh_sampler2 = qmc.LatinHypercube(d=2)\n",
    "    \n",
    "    #update the learning rate as defined\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    #Iterate over time slices\n",
    "    for final_time in time_slices:\n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"\\n\\nCurrent End Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        epoch = 0\n",
    "        for epoch in range(1, iterations):\n",
    "\n",
    "            ##Define input points with LatinHypercube sampling\n",
    "            x_IC = x_l + (x_u-x_l) *IC_lh_sampler.random(n=IC_collocation).reshape(IC_collocation,1)\n",
    "            t_IC = np.random.uniform(low=t_l, high=t_l, size=(IC_collocation,1))\n",
    "            \n",
    "            input_x_IC = Variable(torch.from_numpy(x_IC).float(), requires_grad=True).to(device)\n",
    "            input_t_IC = Variable(torch.from_numpy(t_IC).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            t_BC = t_l + (final_time-t_l) *BC_lh_sampler.random(n=BC_collocation).reshape(BC_collocation,1)\n",
    "    \n",
    "            input_t_BC = Variable(torch.from_numpy(t_BC).float(), requires_grad=True).to(device)\n",
    "    \n",
    "            x_domain = x_l + (x_u-x_l) *PDE_lh_sampler.random(n=pde_collocation)[:,0].reshape(pde_collocation,1)\n",
    "            t_domain = t_l + (final_time-t_l) *PDE_lh_sampler.random(n=pde_collocation)[:,1].reshape(pde_collocation,1)\n",
    "            #t_domain_exp = exp_time_sample(pde_collocation, t_l, final_time, r=10)\n",
    "            \n",
    "            input_x_domain = Variable(torch.from_numpy(x_domain).float(), requires_grad=True).to(device)\n",
    "            input_t_domain = Variable(torch.from_numpy(t_domain).float(), requires_grad=True).to(device)\n",
    "            #input_t_domain_exp = Variable(torch.from_numpy(t_domain_exp).float(), requires_grad=True).to(device)\n",
    "\n",
    "            x_domain_r = x_l + (x_u-x_l) *PDE_lh_sampler2.random(n=pde_collocation)[:,0].reshape(pde_collocation,1)\n",
    "            t_domain_r = t_l + (final_time-t_l) *PDE_lh_sampler2.random(n=pde_collocation)[:,1].reshape(pde_collocation,1)\n",
    "            #t_domain_r_exp = exp_time_sample(pde_collocation, t_l, final_time, r=10)\n",
    "            \n",
    "            input_x_domain_r = Variable(torch.from_numpy(x_domain_r).float(), requires_grad=True).to(device)\n",
    "            input_t_domain_r = Variable(torch.from_numpy(t_domain_r).float(), requires_grad=True).to(device)\n",
    "            #input_t_domain_r_exp = Variable(torch.from_numpy(t_domain_r_exp).float(), requires_grad=True).to(device)\n",
    "            \n",
    "            for i in list(range(0, int(pde_collocation//32)+1)):\n",
    "                # initialize gradients to zero\n",
    "                net.optimizer.zero_grad()\n",
    "                \n",
    "                PDEloss_tensor= lossNSpde_rank(net, input_x_domain_r, input_t_domain_r)     \n",
    "            \n",
    "                sorted_tensor, indices = torch.sort(PDEloss_tensor.view(-1), descending=True)\n",
    "                sorted_tensor = sorted_tensor.view(-1, 1)\n",
    "            \n",
    "                max_stad = sorted_tensor[int(pde_collocation/10)-1,0]\n",
    "            \n",
    "                PDEloss_picked = torch.where(PDEloss_tensor>=max_stad, PDEloss_tensor, 0)\n",
    "                #x_domain_r = torch.where(PDEloss_tensor==PDEloss_picked, input_x_domain_r, 0)\n",
    "                #t_domain_r = torch.where(PDEloss_tensor==PDEloss_picked, input_t_domain_r, 0)\n",
    "            \n",
    "                PDEloss_adaptive = lossNSpde(net, input_x_domain_r, input_t_domain_r)*10 #(torch.sum(PDEloss_picked**2)/(int(pde_collocation/10)))**.5\n",
    "\n",
    "                #Loss computation\n",
    "                u_IC_loss = lossIC(net, input_x_IC, input_t_IC) #, u_IC_loss_mesh\n",
    "                mse_IC = u_IC_loss\n",
    "            \n",
    "                #Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "                mse_BC_u, mse_BC_u_x = lossBdry(net, input_t_BC) \n",
    "                mse_BC = mse_BC_u+ mse_BC_u_x \n",
    "            \n",
    "                #Loss based on PDE\n",
    "                AC_mse= lossNSpde(net, input_x_domain, input_t_domain) \n",
    "                mse_PDE = AC_mse #+ PDEloss_adaptive\n",
    "        \n",
    "                loss =  (mse_BC + mse_IC + mse_PDE )\n",
    "        \n",
    "                loss.backward()\n",
    "            \n",
    "            \n",
    "                def closure():\n",
    "                    return loss\n",
    "            \n",
    "                #Make Iterative Step\n",
    "                net.optimizer.step() #net.optimizer.step(closure)\n",
    "            \n",
    "            \n",
    "            # Gradient Norm Clipping\n",
    "            #torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm = 1000, error_if_nonfinite=False) #note max norm is more art than science\n",
    "            \n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch == 1 or epoch%print_loss == 0:\n",
    "                    print(\"\\nIteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    #,\"\\tadaptive IC Loss2: \", mse_IC_a2.item() , \"\\tadaptive CH PDE Loss2: \", CH_a2.item()\n",
    "                    print(\"\\tIC Loss: \", mse_IC.item(),\n",
    "                          \"\\t BC u Loss: \", mse_BC_u.item(), \"\\t BC u_x Loss: \", mse_BC_u_x.item(),\n",
    "                          \"\\nAC PDE Loss: \", AC_mse.item(), \"\\tadaptive PDE Loss: \", PDEloss_adaptive.item()\n",
    "                         ) \n",
    "                if epoch%6000 == 0:\n",
    "                    np.savetxt(f\"{ctime}epsilon.txt\", epsilon)\n",
    "                    torch.save(net.state_dict(), f\"lr{get_lr(net.optimizer)}_t{final_time}_{ctime}.pt\")\n",
    "        \n",
    "                \n",
    "\n",
    "create_network(load=False, loadfile = \"lr0.001_t1_4_1_15h.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a748d9-0325-4be6-9344-20940f43a2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
